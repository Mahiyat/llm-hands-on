{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO/cgesEipokR6ucHKqfVX2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Import Required Libraries**"],"metadata":{"id":"VuXaMgS087aX"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Anb2p_jT68XJ","executionInfo":{"status":"ok","timestamp":1720586371716,"user_tz":-360,"elapsed":3406,"user":{"displayName":"Mahiyat Tanzim","userId":"03004994999431640800"}},"outputId":"339c41f5-65f7-46a4-9bc1-25ae50d2a99e"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","nltk.download('punkt')\n","nltk.download('wordnet')"]},{"cell_type":"markdown","source":["\n","\n","*   import nltk: Imports the nltk library, which is used for natural language processing.\n","*   from nltk.tokenize import word_tokenize: Imports the word_tokenize function, which is used to split text into words (tokens).\n","*   from nltk.stem import PorterStemmer, WordNetLemmatizer: Imports the PorterStemmer and WordNetLemmatizer classes for stemming and lemmatization, respectively.\n","*   nltk.download('punkt'): Downloads the NLTK data package for tokenization.\n","*   nltk.download('wordnet'): Downloads the NLTK data package for WordNet, which is used for lemmatization.\n","\n","\n"],"metadata":{"id":"cLh1SqMw89OS"}},{"cell_type":"markdown","source":["**Define Text**"],"metadata":{"id":"Ly8guVgE9OBG"}},{"cell_type":"code","source":["text = \"The quick brown foxes jumped over the lazy dogs.\""],"metadata":{"id":"4ugRsc6V7XNg","executionInfo":{"status":"ok","timestamp":1720586371716,"user_tz":-360,"elapsed":6,"user":{"displayName":"Mahiyat Tanzim","userId":"03004994999431640800"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","text: Defines the input text to be processed.\n"],"metadata":{"id":"QnJDW93X9Q5o"}},{"cell_type":"markdown","source":["**Tokenize**"],"metadata":{"id":"3rblLIqx9atZ"}},{"cell_type":"code","source":["tokens = word_tokenize(text)\n","print(\"Tokens: \",tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fGGkBt8t7ft9","executionInfo":{"status":"ok","timestamp":1720586371716,"user_tz":-360,"elapsed":5,"user":{"displayName":"Mahiyat Tanzim","userId":"03004994999431640800"}},"outputId":"17d6d85f-5291-40df-c6e4-8f24c085a8c3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokens:  ['The', 'quick', 'brown', 'foxes', 'jumped', 'over', 'the', 'lazy', 'dogs', '.']\n"]}]},{"cell_type":"markdown","source":["\n","\n","*   tokens = word_tokenize(text): Tokenizes the text into individual words using the word_tokenize function.\n","*   print(\"Tokens: \", tokens): Prints the list of tokens.\n","\n"],"metadata":{"id":"gggPFMsB9j_p"}},{"cell_type":"markdown","source":["**Stemming**"],"metadata":{"id":"2_xeBxCi9so7"}},{"cell_type":"code","source":["stemmer = PorterStemmer()\n","stemmed_tokens = [stemmer.stem(token) for token in tokens]\n","print(\"Stemmed Tokens: \",stemmed_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ky_jbZEQ7kgi","executionInfo":{"status":"ok","timestamp":1720586371716,"user_tz":-360,"elapsed":4,"user":{"displayName":"Mahiyat Tanzim","userId":"03004994999431640800"}},"outputId":"5e95c4bc-7d86-43ab-f722-27102ed1f19a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Stemmed Tokens:  ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog', '.']\n"]}]},{"cell_type":"markdown","source":["\n","\n","*   stemmer = PorterStemmer(): Creates an instance of the PorterStemmer class.\n","*   stemmed_tokens = [stemmer.stem(token) for token in tokens]: Applies stemming to each token using a list comprehension. The stemmer.stem(token) method reduces each token to its stem or root form.\n","*   print(\"Stemmed Tokens: \", stemmed_tokens): Prints the list of stemmed tokens.\n","\n","\n"],"metadata":{"id":"Ie0bJWu_9w6S"}},{"cell_type":"markdown","source":["**Lemmatization**"],"metadata":{"id":"68sHj7Z397GF"}},{"cell_type":"code","source":["lemmatizer = WordNetLemmatizer()\n","lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","print(\"Lemmatized Tokens: \",lemmatized_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1frJZPEF7s1s","executionInfo":{"status":"ok","timestamp":1720586375601,"user_tz":-360,"elapsed":3888,"user":{"displayName":"Mahiyat Tanzim","userId":"03004994999431640800"}},"outputId":"cbba2fe6-83fa-47e5-ea6a-9313abe78901"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Lemmatized Tokens:  ['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', '.']\n"]}]},{"cell_type":"markdown","source":["\n","\n","*   lemmatizer = WordNetLemmatizer(): Creates an instance of the WordNetLemmatizer class.\n","*   lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]: Applies lemmatization to each token using a list comprehension. The lemmatizer.lemmatize(token) method reduces each token to its base or dictionary form.\n","*   print(\"Lemmatized Tokens: \", lemmatized_tokens): Prints the list of lemmatized tokens.\n","\n"],"metadata":{"id":"0DycecGR9-cx"}},{"cell_type":"markdown","source":["**Summary**<br>\n","This notebook processes the input text by tokenizing it into individual words, then applying stemming and lemmatization to the tokens. The output demonstrates the differences between stemming and lemmatization:\n","\n","*  Tokenization: Splits the text \"The quick brown foxes jumped over the lazy dogs.\" into individual words: ['The', 'quick', 'brown', 'foxes', 'jumped', 'over', 'the', 'lazy', 'dogs', '.'].\n","*  Stemming: Reduces each word to its stem form: ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog', '.'].\n","*  Lemmatization: Reduces each word to its base form: ['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', '.']."],"metadata":{"id":"qMtSNjt5-Hv-"}}]}