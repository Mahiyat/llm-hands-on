{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Libraries",
   "id": "ecc084a4759092a1"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-09T17:12:01.305330Z",
     "start_time": "2024-10-09T17:11:56.651209Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* torch: PyTorch is used as the underlying deep learning framework.\n",
    "* T5Tokenizer and T5ForConditionalGeneration are classes from Hugging Face's transformers library.\n",
    "    * T5Tokenizer handles the tokenization of input text.\n",
    "    * T5ForConditionalGeneration is the model class for generating sequences using T5."
   ],
   "id": "fdf564b81d1ad171"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize Models",
   "id": "921f7f3e4e9198f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T17:12:01.321331Z",
     "start_time": "2024-10-09T17:12:01.310331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name1 = \"t5-small\"\n",
    "model_name2 = \"t5-base\"\n",
    "model_name3 = \"t5-large\""
   ],
   "id": "784a6ad612ce66a2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The code initializes three different variations of the T5 model:\n",
    "* t5-small: The smallest version of T5.\n",
    "* t5-base: A medium-sized version.\n",
    "* t5-large: A larger version of the model."
   ],
   "id": "496639175a20790e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize Tokenizers",
   "id": "7bcd38a16bb13869"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T17:12:14.012295Z",
     "start_time": "2024-10-09T17:12:01.402331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer1 = T5Tokenizer.from_pretrained(model_name1)\n",
    "tokenizer2 = T5Tokenizer.from_pretrained(model_name2)\n",
    "tokenizer3 = T5Tokenizer.from_pretrained(model_name3)"
   ],
   "id": "c8cff472ccb8926e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3e16c78be14c464c8f5b02e590cd8c77"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac961e74dbd24f789f5f3224a64f90d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c0e084c5e2b4976b8385fff8021d8f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "672b45b295134152900a8167e27036b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jucse\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f35d98bad154b23bd11280977a565fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19fa33b357314bdd806d58162724f8d4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jucse\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For each model (small, base, large), a corresponding tokenizer is loaded.",
   "id": "bf3df4bc15baa2c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Models",
   "id": "ddd82e126ce53e61"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T17:54:25.002538Z",
     "start_time": "2024-10-09T17:12:14.227345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model1 = T5ForConditionalGeneration.from_pretrained(model_name1)\n",
    "model2 = T5ForConditionalGeneration.from_pretrained(model_name2)\n",
    "model3 = T5ForConditionalGeneration.from_pretrained(model_name3)"
   ],
   "id": "7d0f0b047b130a82",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2853d1f6ebf243daad7a5d0f2fbf34fe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e2e20d36d6d41bd864b328e5a129f5d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0fee023c1c524792bac9ce6b346a4bba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55e9b5a916274dd682112eb381436ca5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For each model (small, base, large), model is loaded using from_pretrained.",
   "id": "aa5afb45de45eb0e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenize the Input Prompt",
   "id": "d165c7ce610b028e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T17:54:25.137089Z",
     "start_time": "2024-10-09T17:54:25.105759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_prompt = \"Generate a sentence that describes the importance of Machine Learning.\"\n",
    "input_ids1 = tokenizer1.encode(input_prompt, return_tensors='pt')\n",
    "input_ids2 = tokenizer2.encode(input_prompt, return_tensors='pt')\n",
    "input_ids3 = tokenizer3.encode(input_prompt, return_tensors='pt')"
   ],
   "id": "a722b8ac43989111",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Input Prompt: The prompt for all models is \"Generate a sentence that describes the importance of Machine Learning.\".\n",
    "* The encode() method tokenizes the input prompt into token IDs and returns tensors (return_tensors='pt'), ready for PyTorch models."
   ],
   "id": "26876567b03ddeaa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generate Text from Each Model",
   "id": "386dda2c0089b327"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T17:54:54.294877Z",
     "start_time": "2024-10-09T17:54:25.186682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output1 = model1.generate(\n",
    "    input_ids1, max_length=50, num_return_sequences=1, pad_token_id=tokenizer1.eos_token_id\n",
    ")\n",
    "output2 = model2.generate(\n",
    "    input_ids2, max_length=50, num_return_sequences=1, pad_token_id=tokenizer2.eos_token_id\n",
    ")\n",
    "output3 = model3.generate(\n",
    "    input_ids3, max_length=50, num_return_sequences=1, pad_token_id=tokenizer3.eos_token_id\n",
    ")"
   ],
   "id": "c18d195cf7f99569",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Text Generation: The generate() method is used to produce text from the model:\n",
    "* input_ids1, input_ids2, and input_ids3 are the tokenized prompts passed as input.\n",
    "* max_length=50: The generated text is limited to a maximum length of 50 tokens.\n",
    "* num_return_sequences=1: Only 1 sequence is generated.\n",
    "* pad_token_id=tokenizerX.eos_token_id: Padding is done using the end-of-sequence (EOS) token."
   ],
   "id": "c58784d9c80b716d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Decode the Generated Text",
   "id": "5bda91cf3fcb36de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T17:54:54.378401Z",
     "start_time": "2024-10-09T17:54:54.327764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generated_text1 = tokenizer1.decode(output1[0], skip_special_tokens=True)\n",
    "generated_text2 = tokenizer2.decode(output2[0], skip_special_tokens=True)\n",
    "generated_text3 = tokenizer3.decode(output3[0], skip_special_tokens=True)"
   ],
   "id": "cbbee82fed154735",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The generated token sequences (output1, output2, output3) are decoded back into human-readable text using the tokenizer's decode() method. The skip_special_tokens=True option removes special tokens like pad or eos from the final output.",
   "id": "1d57e4fc94dc17fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Print the Results",
   "id": "cafc5847d2c59de7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T17:54:54.425200Z",
     "start_time": "2024-10-09T17:54:54.409681Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Generated Text by t5-small model:\", generated_text1)",
   "id": "ca5b87473924209a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text by t5-small model: Generieren Sie eine Satz, die die Beschreibung der Bedeutung von Machine Learning.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T17:54:54.472076Z",
     "start_time": "2024-10-09T17:54:54.456464Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Generated Text by t5-base model:\", generated_text2)",
   "id": "6a42ca63dfea9d2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text by t5-base model: a sentence that describes the importance of Machine Learning. Generate a sentence that describes the importance of Machine Learning.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T17:54:54.518954Z",
     "start_time": "2024-10-09T17:54:54.503400Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Generated Text by t5-large model:\", generated_text3)",
   "id": "c9bcb3499ccdf5b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text by t5-large model: a sentence that describes the importance of Machine Learning... Describe Machine Learning....rate a sentence that describes the importance of Machine Learning.. a sentence that describes\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The generated text from each model (small, base, large) is printed for comparison.",
   "id": "c5e1fc53ca94a7d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "This code compares the performance of three different T5 models (small, base, and large) in generating text based on the same input prompt. The models take the prompt \"Generate a sentence that describes the importance of Machine Learning\" and generate a response. The t5-small, t5-base, and t5-large models differ in size, computational resources, and ability to generate more complex and accurate text. The comparison helps observe how model size affects the quality of text generation."
   ],
   "id": "6cdbccb7be2011ca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
