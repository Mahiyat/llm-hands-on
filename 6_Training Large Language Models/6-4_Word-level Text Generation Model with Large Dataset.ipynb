{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Tensorflow and Check Version",
   "id": "53695491ff3f1806"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T16:16:21.460573Z",
     "start_time": "2024-10-09T16:16:18.460809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ],
   "id": "ea05b8012464cbe0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Necessary Libraries",
   "id": "bb2f29ffca17966b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-09T16:16:21.566695Z",
     "start_time": "2024-10-09T16:16:21.533761Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import requests"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The code imports necessary libraries:\n",
    "* numpy for numerical operations.\n",
    "* Tokenizer and pad_sequences from tensorflow.keras.preprocessing.text to tokenize and pad sequences of text.\n",
    "* requests for fetching data from a URL."
   ],
   "id": "b35ce0c71c63e3e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fetch Shakespeare Data",
   "id": "f8a8d67b5861e592"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T16:16:22.477556Z",
     "start_time": "2024-10-09T16:16:21.629285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = requests.get('https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt')\n",
    "shakespeare_data = response.text"
   ],
   "id": "ffae67789d9e44fa",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The code requests to get a dataset of Shakespeare's text from the provided URL.",
   "id": "68690566e0d4c3b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Store the Fetched Data",
   "id": "cee48f69a1eb9c41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T16:16:22.524436Z",
     "start_time": "2024-10-09T16:16:22.508869Z"
    }
   },
   "cell_type": "code",
   "source": "input_text = shakespeare_data",
   "id": "72a38700d1c2a805",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The code assigns the fetched data in input_text",
   "id": "3a4deac74aa243e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenization",
   "id": "da0f835cdbf98a36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T16:16:22.626385Z",
     "start_time": "2024-10-09T16:16:22.540060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([input_text])"
   ],
   "id": "7d649c248fdb295e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A tokenizer object is created, and it is trained on the Shakespearean text. It converts words into a numeric representation.",
   "id": "8818d9faee051149"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Calculate Number of Words",
   "id": "945f8ab58eac687f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T16:16:22.657524Z",
     "start_time": "2024-10-09T16:16:22.641987Z"
    }
   },
   "cell_type": "code",
   "source": "num_words = len(tokenizer.word_index) + 1",
   "id": "b6d0efe198691d9f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "num_words is calculated based on the number of unique words in the text, plus 1 for padding.",
   "id": "cf3d3f1a28b68875"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Input Sequence",
   "id": "c635e1e70ae24c35"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T16:16:22.735739Z",
     "start_time": "2024-10-09T16:16:22.673152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_sequences = tokenizer.texts_to_sequences([input_text])\n",
    "input_seq_length = 10\n",
    "step = 1"
   ],
   "id": "7c3a70c7b345d03b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Input Sequences: The text is tokenized into a list of integers using texts_to_sequences.",
   "id": "10d173aecf53cbf0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T16:16:23.032577Z",
     "start_time": "2024-10-09T16:16:22.751340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = []\n",
    "y = []\n",
    "for i in range(0, len(input_sequences[0])-input_seq_length, step):\n",
    "  input_seq = input_sequences[0][i:i+input_seq_length]\n",
    "  output_seq = input_sequences[0][i+input_seq_length]\n",
    "  X.append(input_seq)\n",
    "  y.append(output_seq)"
   ],
   "id": "85e2a3c2ae1ef9a8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Sliding Window: A sliding window of length 10 (input_seq_length) moves over the tokenized text with step size 1 (step), generating overlapping sequences. These sequences represent the training data (X).\n",
    "* Next Word Prediction: For each input sequence, the next word in the text (y) is used as the output label."
   ],
   "id": "2c392c7f0635122d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T16:16:23.578469Z",
     "start_time": "2024-10-09T16:16:23.048203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "y = to_categorical(y, num_classes=num_words)"
   ],
   "id": "f7265d2a1fb3e434",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The training data X and output label y are converted to numpy arrays.\n",
    "* The labels y are then one-hot encoded using to_categorical, converting the output into a vector format."
   ],
   "id": "939770522647f1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define the Model",
   "id": "7ac567ea389dd0d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T16:16:23.690552Z",
     "start_time": "2024-10-09T16:16:23.596739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=num_words,output_dim=100,input_length=input_seq_length),\n",
    "    LSTM(128),\n",
    "    Dense(num_words, activation='softmax')\n",
    "])"
   ],
   "id": "7cfcb52d57af1a1a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jucse\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Embedding Layer: This converts each word in the input sequence into a 100-dimensional vector representation. The input_dim is num_words (the vocabulary size), and the input_length is input_seq_length.\n",
    "* LSTM Layer: A layer with 128 hidden units to process the sequential data and capture long-term dependencies in the text.\n",
    "* Dense Layer: A dense output layer with num_words neurons and a softmax activation function, which outputs the probability distribution over the vocabulary for the next word."
   ],
   "id": "d977b5ee8172b2f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Compile the Model",
   "id": "27bc3e970f403e67"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T16:16:23.721828Z",
     "start_time": "2024-10-09T16:16:23.706193Z"
    }
   },
   "cell_type": "code",
   "source": "model.compile(loss='categorical_crossentropy', optimizer='adam')",
   "id": "979054e70aa95531",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The model is compiled with the categorical crossentropy loss function and the Adam optimizer.",
   "id": "ee2ca1e756830e7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Print the Training Data and Output Label Shapes",
   "id": "424edd9a9eba1513"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T16:16:23.753070Z",
     "start_time": "2024-10-09T16:16:23.737454Z"
    }
   },
   "cell_type": "code",
   "source": "print(X.shape, y.shape)",
   "id": "a14fb0abcbc896c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204079, 10) (204079, 12633)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The shapes of X and y are printed for observation.",
   "id": "d6a4190842f6f987"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train the Model",
   "id": "105b029f87a4e0ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T16:22:22.020069Z",
     "start_time": "2024-10-09T16:16:23.768694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 10\n",
    "batch_size = 4096\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size)"
   ],
   "id": "845187762dcfe2f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 555ms/step - loss: 8.8226\n",
      "Epoch 2/10\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 564ms/step - loss: 6.8037\n",
      "Epoch 3/10\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 496ms/step - loss: 6.7886\n",
      "Epoch 4/10\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 555ms/step - loss: 6.7882\n",
      "Epoch 5/10\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m28s\u001B[0m 484ms/step - loss: 6.7687\n",
      "Epoch 6/10\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 504ms/step - loss: 6.7165\n",
      "Epoch 7/10\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m28s\u001B[0m 503ms/step - loss: 6.6499\n",
      "Epoch 8/10\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 541ms/step - loss: 6.5891\n",
      "Epoch 9/10\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m28s\u001B[0m 531ms/step - loss: 6.5316\n",
      "Epoch 10/10\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 557ms/step - loss: 6.4849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1ab8999ff40>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The model is trained on the input data X and corresponding output y for 10 epochs with a batch size of 4096. The goal is for the model to predict the next word given a sequence of 10 words.",
   "id": "e23cdab47e5e2801"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Text Generation",
   "id": "87cb470f41f9dd7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T16:22:22.176274Z",
     "start_time": "2024-10-09T16:22:22.160638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_text(model, seed_text, num_words_to_generate=50):\n",
    "  generated_text = seed_text\n",
    "  for _ in range(num_words_to_generate):\n",
    "      input_seq = tokenizer.texts_to_sequences([generated_text])[0]\n",
    "      input_seq = pad_sequences([input_seq], maxlen=input_seq_length, padding='pre')\n",
    "\n",
    "      preds = model.predict(input_seq)[0]\n",
    "      next_word_idx = np.argmax(preds)\n",
    "      next_word = tokenizer.index_word[next_word_idx]\n",
    "      generated_text += \" \" + next_word\n",
    "  return generated_text"
   ],
   "id": "2a06454572a981e2",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The generate_text function generates new text based on a seed text.\n",
    "* The seed text is tokenized and padded to match the input sequence length.\n",
    "* The trained model predicts the probability distribution of the next word.\n",
    "* The most probable next word is chosen (np.argmax(preds)), and it is added to the generated text.\n",
    "* The process repeats until num_words_to_generate new words have been generated."
   ],
   "id": "ed0d3cf2a61412e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run Text Generation:",
   "id": "fc0842824881dd78"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T16:25:02.161213Z",
     "start_time": "2024-10-09T16:24:59.847621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "seed_text = \"To be or not to be\"\n",
    "generated_text = generate_text(model, seed_text, num_words_to_generate=50)\n",
    "print(generated_text)"
   ],
   "id": "bba8eec0f5340e8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 526us/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 31ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 31ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 31ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "To be or not to be the lord and the lord and the lord and the lord and the lord and the lord and the lord and the lord and the lord and the lord and the lord and the lord and the lord and the lord and the lord and the lord and the lord\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* A seed text is provided (\"To be or not to be\"), and the model generates 50 more words based on this seed.\n",
    "* The generated text is printed."
   ],
   "id": "d9724c836f460dbf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "This code builds a word-based LSTM language model that learns from Shakespeare’s text and can generate new text. After tokenizing the data, it creates input-output sequences and trains the model. The model can then generate text given a seed, by predicting the next word in the sequence."
   ],
   "id": "ee8ae08a2578211f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Limitations of the Model\n",
    "* Limited context window leading to more superficial text generation\n",
    "* Word-level tokenization which can be limiting when dealing with rare or unseen words\n",
    "* Comparatively smaller and simple than existing transformer-based pre-trained models"
   ],
   "id": "1f06a2c8175c66fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ways to Improve\n",
    "* Using a large dataset with a variety of contexts\n",
    "* Designing complex model architecture\n",
    "* Using transformer-based models"
   ],
   "id": "8d97fc8394d9903a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
