{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pre-requisite\n",
    "**Setup Ollama**<br>\n",
    "For setting up ollama in your local environment, click [here](https://github.com/ollama/ollama).\n",
    "\n",
    "**Pull Model**<br>\n",
    "Pull the required model running the following command:\n",
    "```bash\n",
    "ollama pull llama3.2\n",
    "```\n",
    "To use more variants of the llama model check [here](https://ollama.com/library).\n",
    "\n",
    "**Install python library**<br>\n",
    "Install the python library for llama_index\n",
    "```bash\n",
    "pip install llama-index\n",
    "pip install llama-index-embeddings-huggingface\n",
    "pip install llama-index-llm-ollama\n",
    "```\n",
    "To learn more, click [here](https://docs.llamaindex.ai/en/stable/)."
   ],
   "id": "483b066005cac94c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Required Libraries",
   "id": "700d95f140937408"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-17T12:06:25.671878Z",
     "start_time": "2024-10-17T12:06:10.370710Z"
    }
   },
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import PromptTemplate"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jucse\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* SimpleDirectoryReader: Used for reading documents (specifically PDFs in this case) from a directory.\n",
    "* HuggingFaceEmbedding: Interface to use pre-trained models from Hugging Face for generating embeddings (numerical representations of text).\n",
    "* Settings: A configuration object where we can store models such as the LLM and embeddings for use later.\n",
    "* VectorStoreIndex: Creates an index where documents are stored as vectors. This index will allow querying based on document similarity.\n",
    "* Ollama: A class to load and use a large language model (LLM) for answering queries.\n",
    "* PromptTemplate: Used to customize the prompt format for the LLM's response."
   ],
   "id": "d0fa41359239d19e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the Knowledge Base (PDF Documents)",
   "id": "eb4b55ce858728f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T12:06:43.477792Z",
     "start_time": "2024-10-17T12:06:25.686885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_dir_path = \"./pdf_directory\"\n",
    "\n",
    "loader = SimpleDirectoryReader(\n",
    "            input_dir = input_dir_path,\n",
    "            required_exts=[\".pdf\"],\n",
    "            recursive=True\n",
    "        )\n",
    "docs = loader.load_data()"
   ],
   "id": "a82a608fc32ea35b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* input_dir_path: Specifies the path where the PDF files are stored.\n",
    "* SimpleDirectoryReader: Reads files from the directory specified. In this case, only files with the .pdf extension are processed (required_exts=[\".pdf\"]), and subdirectories are also searched (recursive=True).\n",
    "* docs: After reading, the documents are loaded into memory and stored in the docs variable. These documents will later be embedded for similarity-based search.\n",
    "\n",
    "Here, the knowledge base consists of research papers on LLM in Cybersecurity."
   ],
   "id": "1dbac66bb5fd425f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Embedding Model",
   "id": "445ddaea068ba16e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T12:20:13.755372Z",
     "start_time": "2024-10-17T12:06:43.759677Z"
    }
   },
   "cell_type": "code",
   "source": "embed_model = HuggingFaceEmbedding( model_name=\"BAAI/bge-large-en-v1.5\", trust_remote_code=True)",
   "id": "e110ac5d8c672418",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ceb7ce8b328479597b716bd84de0bfd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8434c96bbc1e4a74801e5e7a268d46de"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/94.6k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab069cc4bb3a4075bf8ecb717616bd83"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac1a811eb985409389020e7b2f300919"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/779 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc36cb5a726d4b7c9eb12203564239c6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "565a9f2b03af47e6b805ac28bbf4d5e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a365f2f1282f45dab46b1d81f80de67a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "965adc5e8c03446aadecdaa8a51874dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c05f5f85f35b4012bb56e32939b9e577"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce06c96b23b54e59b31c7782532065e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf162add32214f02a532360b276c17e8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* HuggingFaceEmbedding: The chosen model for embedding is \"BAAI/bge-large-en-v1.5\", which is a large pre-trained embedding model that converts documents into embeddings (numerical vectors representing the semantic meaning of the text).\n",
    "* trust_remote_code=True: This allows code from Hugging Face's model repository to be safely trusted and executed."
   ],
   "id": "2b3eacebe409245e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Embed the Documents and Create a Vector Index",
   "id": "d2a5720f6fe8bf18"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T12:21:16.254751Z",
     "start_time": "2024-10-17T12:20:13.765175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Settings.embed_model = embed_model\n",
    "index = VectorStoreIndex.from_documents(docs)"
   ],
   "id": "19bae0f48a83d330",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jucse\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Settings.embed_model: The embedding model is assigned to Settings, making it accessible throughout the rest of the pipeline.\n",
    "* VectorStoreIndex: This creates a vector-based index from the embedded documents. This index allows for efficient similarity-based searches using embeddings.\n",
    "* from_documents(docs): Embeds the documents and stores them in a searchable vector format."
   ],
   "id": "a90a53a4f7eda78b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup the LLM and Configure the Query Engine",
   "id": "a2035be1675ef32e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T12:21:16.268323Z",
     "start_time": "2024-10-17T12:21:16.264120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = Ollama(model=\"llama3.2\", request_timeout=120.0) \n",
    "\n",
    "Settings.llm = llm\n",
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k=4)"
   ],
   "id": "e903f6054e0bc61e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Ollama: This loads the LLM, in this case, the \"llama3.2\" model, which is capable of generating text-based answers to queries.\n",
    "* request_timeout=120.0: This sets a timeout of 120 seconds for generating a response from the LLM.\n",
    "* Settings.llm: The LLM is stored in Settings for later use by the query engine.\n",
    "* index.as_query_engine(): Converts the vector index into a query engine that allows you to retrieve relevant documents.\n",
    "* streaming=True: The results are streamed progressively, so that there is no need to wait for the entire response.\n",
    "* similarity_top_k=4: The query engine will return the top 4 most similar documents to the user’s query based on the vector embeddings."
   ],
   "id": "4fb48870d6e64d5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create a Custom Prompt Template",
   "id": "2d97505abc6d6122"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T12:21:16.280409Z",
     "start_time": "2024-10-17T12:21:16.276568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "qa_prompt_tmpl_str = (\n",
    "            \"Context information is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information above I want you to think step by step to answer the query in a crisp manner, in case you don't know the answer say 'I don't know!'.\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"Answer: \"\n",
    "            )\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_prompt_tmpl})"
   ],
   "id": "a7e0f741dba213e4",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* qa_prompt_tmpl_str: This is the string template for the LLM to follow when answering queries. It instructs the model to use the context (retrieved documents) to answer the query and to be clear in the response.\n",
    "    * {context_str}: This placeholder will be filled with relevant document context retrieved from the vector index.\n",
    "    * {query_str}: This placeholder will be filled with the user’s question.\n",
    "* PromptTemplate: This wraps the string into a template that can be applied to each query.\n",
    "* update_prompts(): The query engine's prompt for text-based question answering is updated with this custom prompt template."
   ],
   "id": "20954e55782b38c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Query the Index",
   "id": "a65ce2252b88847c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T12:21:32.267824Z",
     "start_time": "2024-10-17T12:21:16.297083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response1 = query_engine.query('How can we use LLM in cybersecurity?')\n",
    "print(response1)"
   ],
   "id": "e84a4f0d828c5587",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context information and the review paper's findings, here is a concise summary of how LLMs can be used in cybersecurity:\n",
      "\n",
      "LLMs can be utilized in various cybersecurity tasks, including:\n",
      "\n",
      "1. **Threat Intelligence**: Assisting in generating and analyzing Cyber Threat Intelligence (CTI), assisting in security response, and identifying potential threats.\n",
      "2. **Vulnerability Detection**: Improving detection capabilities through prompt engineering and vulnerability dataset preparation.\n",
      "3. **Secure Code Generation**: Evaluating the security of LLM-generated code, enhancing its security, and providing static analysis assistants.\n",
      "4. **Program Repair**: Evaluating LLMs' program repair capability, combining LLMs with static analysis tools, and improving through different strategies.\n",
      "5. **Anomaly Detection**: Assisting in log-based anomaly detection, web content security, digital forensic analysis, and other areas.\n",
      "6. **LLM-Assisted Attack**: Enabling LLM-assisted privilege escalation attacks, CTF challenges, phishing website/email generation, payload generation, automated penetration testing, and more.\n",
      "\n",
      "Additionally, the review paper highlights the potential applications of LLMs in cybersecurity, including:\n",
      "\n",
      "* Enhancing threat detection\n",
      "* Automating vulnerability analysis\n",
      "* Providing intelligent defense mechanisms\n",
      "* And others\n",
      "\n",
      "To construct cybersecurity-oriented domain LLMs, researchers can use methods such as CPT (Continuous Pre-training) and SFT (Self-supervised Fine-tuning).\n",
      "\n",
      "Overall, the potential applications of LLMs in cybersecurity are vast and varied, offering promising results for enhancing cybersecurity practices.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T12:21:41.696216Z",
     "start_time": "2024-10-17T12:21:32.356035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response2 = query_engine.query('What are the roles of Rule-based AI in designing critical infrastructure?')\n",
    "print(response2)"
   ],
   "id": "1c6eae27d7761201",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, here is a step-by-step analysis:\n",
      "\n",
      "1. The context information discusses the importance of Critical Infrastructure (CI) and the need for effective cybersecurity solutions to protect it.\n",
      "2. It mentions that Rule-based AI modeling has been identified as a promising approach in enhancing CI security solutions, particularly in terms of automation, intelligence, and transparency.\n",
      "3. According to the article, Rule-based AI models can provide human-interpretable decisions, explainability, and trustworthiness in decision-making, which is essential in cybersecurity application areas.\n",
      "\n",
      "Considering these points, I would answer that:\n",
      "\n",
      "The roles of Rule-based AI in designing critical infrastructure include:\n",
      "- Providing transparent and interpretable decisions for cybersecurity applications.\n",
      "- Enabling automation and intelligence in threat detection, mitigation, prediction, diagnosis, and response.\n",
      "- Enhancing explainability analysis to detect insider threats or suspicious activities.\n",
      "- Assisting in protecting various sectors of CI, such as energy, defence, transport, health, water, agriculture, etc.\n",
      "\n",
      "These roles are based on the understanding that Rule-based AI models can be used to monitor user activity, access patterns, and network traffic to identify potential security threats and provide proactive measures to protect critical infrastructure.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* query(): This function sends a query to the vector index, retrieves the top-k similar documents (top 4 in this case), and then passes them to the LLM to generate a response.\n",
    "* response1: This query asks, \"How can we use LLM in cybersecurity?\", and retrieves the relevant context from the PDFs before generating an answer.\n",
    "* print(response1): The first response is printed out.\n",
    "* response2: This query asks, \"What are the roles of Rule-based AI in designing critical infrastructure?\", and retrieves the relevant documents to formulate an answer.\n",
    "* print(response2): The second response is printed out."
   ],
   "id": "229456803132b67a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "The code sets up a Retrieval-Augmented Generation (RAG) pipeline using PDFs. It embeds the documents into vector space using Hugging Face models, creates a vector index, and queries the index using an LLM with a custom prompt template. When a user provides a question, the system retrieves relevant context from the documents and uses the LLM to generate a step-by-step answer based on that context.\n",
    "\n",
    "To learn more visit [here](https://lightning.ai/lightning-ai/studios/rag-using-llama-3-2-by-meta-ai)."
   ],
   "id": "bc9ad6856e2288c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
