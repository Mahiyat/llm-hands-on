{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Login to Hugging Face Hub",
   "id": "51400216e8edaabc"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-14T11:49:08.924306Z",
     "start_time": "2024-12-14T11:49:08.397266Z"
    }
   },
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7037e7c659f44380812b4908ceba70fe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Purpose: Logs into the Hugging Face Hub for access to pre-trained models.\n",
    "* Interaction: The user is prompted to authenticate using their Hugging Face credentials."
   ],
   "id": "96b32b2976888bbe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set Up the Device for Computation",
   "id": "5f7667b0008b3357"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check GPU Availability",
   "id": "b247066aad67fd4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:49:12.841837Z",
     "start_time": "2024-12-14T11:49:08.930312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "id": "53b6bb7e69c280f8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Device Setup",
   "id": "7e9adffdc65e5ac0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:49:12.931319Z",
     "start_time": "2024-12-14T11:49:12.924767Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.set_device(0)",
   "id": "ffc55456852c9c66",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This selects the GPU (if available) for running computations, optimizing performance for large models.",
   "id": "ef126cd1f2f4c042"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Required Libraries",
   "id": "58669e569003f41f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:49:22.070109Z",
     "start_time": "2024-12-14T11:49:13.423395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import LLMChain, StuffDocumentsChain\n",
    "from transformers import AutoTokenizer, pipeline"
   ],
   "id": "35f852a64c4f477d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* PyPDFDirectoryLoader: Loads and preprocesses PDF documents from a specified directory.\n",
    "* Qdrant: Stores and retrieves document embeddings for similarity-based search.\n",
    "* PromptTemplate: Defines structured templates for generating prompts with dynamic placeholders.\n",
    "* HuggingFacePipeline: Integrates Hugging Face's LLM pipelines into LangChain workflows.\n",
    "* HuggingFaceEmbeddings: Converts text into dense vector embeddings for retrieval tasks.\n",
    "* RetrievalQA: Chains document retrieval and LLM-based question answering.\n",
    "* RecursiveCharacterTextSplitter: Splits long texts into smaller, context-preserving chunks.\n",
    "* LLMChain: Links prompts with an LLM for structured response generation.\n",
    "* StuffDocumentsChain: Combines retrieved documents into a single context for LLM processing.\n",
    "* AutoTokenizer: Tokenizes text to prepare it for input into a Hugging Face model.\n",
    "* pipeline: Configures and executes text-generation tasks using Hugging Face models."
   ],
   "id": "386f69db48a501a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load PDF Documents",
   "id": "11115e5f75d0338b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:49:42.769445Z",
     "start_time": "2024-12-14T11:49:22.077124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loader = PyPDFDirectoryLoader(\"./pdf_directory\")\n",
    "docs = loader.load()\n",
    "print(len(docs))"
   ],
   "id": "f6b12232bfd5e3f2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Purpose: Reads all .pdf files from the specified directory and loads them as documents.\n",
    "* Output: The variable docs contains a list of document objects, and len(docs) prints the total number of documents loaded."
   ],
   "id": "55e56d98d833a6b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set Up the Embedding Model",
   "id": "5900f7c11a3369b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:49:52.051405Z",
     "start_time": "2024-12-14T11:49:42.778863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
   ],
   "id": "542588d66427ddde",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jucse\\AppData\\Local\\Temp\\ipykernel_24744\\807396544.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jucse\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Purpose: Initializes the HuggingFaceEmbeddings model to convert text into vector representations (embeddings).\n",
    "* Model Used: sentence-transformers/all-mpnet-base-v2, a widely-used transformer for embeddings."
   ],
   "id": "e0ef353342d6dd0b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Split Documents into Chunks",
   "id": "806799107917ad33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:49:52.131804Z",
     "start_time": "2024-12-14T11:49:52.059739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
    "all_splits = text_splitter.split_documents(docs)"
   ],
   "id": "6ed31c2fc81221fc",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Purpose: Splits large documents into smaller chunks of 500 characters for more effective indexing and retrieval.",
   "id": "bdc6a7cbefec4402"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create a Qdrant Vector Store",
   "id": "1052512b2a395058"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:50:32.247217Z",
     "start_time": "2024-12-14T11:49:52.139057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "qdrant_collection = Qdrant.from_documents(\n",
    "    all_splits,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"responses\",\n",
    ")"
   ],
   "id": "bc9a063cf4fd02fa",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Purpose: Stores the embedded document chunks in a Qdrant vector database, enabling similarity-based retrieval.",
   "id": "ae54072a304220b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configure a Retriever",
   "id": "9a12cfe73af2b467"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:50:32.259517Z",
     "start_time": "2024-12-14T11:50:32.256245Z"
    }
   },
   "cell_type": "code",
   "source": "qdrant_retriever = qdrant_collection.as_retriever()",
   "id": "5e779b9af1f80647",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Purpose: Configures the retriever to fetch documents from the Qdrant database based on a query's similarity to the stored embeddings.",
   "id": "eb6d32ba8e732618"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load and Configure the LLM from Hugging Face",
   "id": "94aa0c6c8970547c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:50:41.606253Z",
     "start_time": "2024-12-14T11:50:32.268598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"google/gemma-2-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cuda\",\n",
    "    max_new_tokens=512\n",
    ")"
   ],
   "id": "60d9b689bd5b71b7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c088ca257e044f0d909e48a4153fc7c9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Purpose: Initializes the Gemma-2 LLM for text generation using the Hugging Face pipeline.\n",
    "* Model Used: google/gemma-2-2b-it."
   ],
   "id": "3933c7089c5e1c8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Test the LLM with Chat-Style Prompts",
   "id": "6d3cfe57f15e0599"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:51:23.388581Z",
     "start_time": "2024-12-14T11:50:41.647966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Where is Milan?\"},\n",
    "]\n",
    "prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipeline(\n",
    "\tprompt,\n",
    "\tmax_new_tokens=256,\n",
    "\tadd_special_tokens=True,\n",
    "\tdo_sample=True,\n",
    "\ttemperature=0.7,\n",
    "\ttop_k=50,\n",
    "\ttop_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ],
   "id": "ffb22d56f4234ce2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'max_batch_size' argument of HybridCache is deprecated and will be removed in v4.46. Use the more precisely named 'batch_size' argument instead.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milan is a city located in **Northern Italy**. \n",
      "\n",
      "More specifically:\n",
      "\n",
      "* **Region:** Lombardy\n",
      "* **Country:** Italy\n",
      " \n",
      "It's the capital of the Lombardy region and is one of the major fashion, economic, and cultural centers of Italy. \n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Purpose: Tests the LLM with a chat-style prompt asking a geographical question.\n",
    "* Steps:\n",
    "    * Constructs a chat-based input format for the model.\n",
    "    * Generates a response using the LLM with specified parameters like temperature (controls randomness) and top-k/top-p sampling (controls diversity)."
   ],
   "id": "9b7b4f15b9527086"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define the Prompt Template",
   "id": "e63e29def0ebc068"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:51:23.471394Z",
     "start_time": "2024-12-14T11:51:23.466737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "qa_prompt_tmpl_str = (\n",
    "            \"Context information is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information above I want you to think step by step to answer the query in a crisp manner, in case you don't know the answer say 'I don't know!'.\\n\"\n",
    "            \"Query: {question}\\n\"\n",
    "            \"Answer: \"\n",
    "            )\n",
    "prompt_template = PromptTemplate.from_template(qa_prompt_tmpl_str)"
   ],
   "id": "c06fef6a722836ec",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Purpose: Defines a chain-of-thought style prompt template that:\n",
    "* Provides context.\n",
    "* Asks the model to answer step-by-step.\n",
    "* Prompts the model to explicitly state \"I don't know!\" if unsure."
   ],
   "id": "d2a285129e97a574"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create the LLM Chain and Combine Documents with the LLM Chain",
   "id": "ec6f0613aa2085bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:51:23.746153Z",
     "start_time": "2024-12-14T11:51:23.552401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gemma_llm = HuggingFacePipeline(\n",
    "    pipeline=pipeline,\n",
    "    model_kwargs={\"temperature\": 0.7},\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=gemma_llm, prompt=prompt_template)\n",
    "\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_variable_name=\"context\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(llm_chain.input_keys)  \n",
    "print(combine_documents_chain.input_keys)  "
   ],
   "id": "e01c8a5d8dce9759",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['context', 'question']\n",
      "['input_documents', 'question']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jucse\\AppData\\Local\\Temp\\ipykernel_24744\\3249647557.py:1: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  gemma_llm = HuggingFacePipeline(\n",
      "C:\\Users\\jucse\\AppData\\Local\\Temp\\ipykernel_24744\\3249647557.py:6: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=gemma_llm, prompt=prompt_template)\n",
      "C:\\Users\\jucse\\AppData\\Local\\Temp\\ipykernel_24744\\3249647557.py:8: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain/\n",
      "  combine_documents_chain = StuffDocumentsChain(\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Purpose:\n",
    "* Connects the Gemma LLM with the defined prompt template to form a processing pipeline.\n",
    "* Processes the retrieved documents to form a cohesive input (context) for the LLM."
   ],
   "id": "fe2c13a7c92fe1e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set Up the Retrieval-QA Pipeline and Query the Retrieval-QA System",
   "id": "89116f8916451628"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:55:21.170282Z",
     "start_time": "2024-12-14T11:51:23.757162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "qa = RetrievalQA(\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    retriever=qdrant_retriever,\n",
    "    verbose=True,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "print(qa.input_keys)\n",
    "\n",
    "query_str = \"How can we use LLM in cybersecurity?\"\n",
    "response = qa.invoke(query_str)\n",
    "\n",
    "print(response[\"result\"])\n"
   ],
   "id": "32389c2a1673da2c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jucse\\AppData\\Local\\Temp\\ipykernel_24744\\1815635984.py:1: LangChainDeprecationWarning: This class is deprecated. Use the `create_retrieval_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/retrieval_qa/\n",
      "  qa = RetrievalQA(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['query']\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new StuffDocumentsChain chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Context information is below.\n",
      "---------------------\n",
      "Table 1: The main cybersecurity tasks and applications where LLMs have been utility.\n",
      "Vulnerability\n",
      "Detection\n",
      "(In)secure\n",
      "Code\n",
      "Generation\n",
      "Program\n",
      "Repairing Binary IT\n",
      "Operations\n",
      "Threat\n",
      "Intelligence\n",
      "Anomaly\n",
      "Detection\n",
      "LLM\n",
      "Assisted\n",
      "Attack\n",
      "Others\n",
      "RQ1 ✓ ✓ ✓ ✓ ✓ - - - ✓\n",
      "RQ2 ✓ ✓ ✓ - - ✓ ✓ ✓ ✓\n",
      "RQ3 - - - - ✓ - ✓ ✓ -\n",
      "Despite initial efforts of LLMs in cybersecurity, the field faces several challenges [17, 31]. First, many studies rely on\n",
      "\n",
      "importance of LLMs in the field of cybersecurity, although the researches of LLMs’ application in their fields\n",
      "are less.\n",
      "3 RQ1: How to construct cybersecurity-oriented domain LLMs?\n",
      "The cybersecurity domain is facing escalating threats, demanding intelligent and efficient solutions to counter sophisti-\n",
      "cated and evolving attacks [37, 38, 39]. LLMs offers novel opportunities for the cybersecurity community [ 18, 19].\n",
      "\n",
      "decisions and prioritize responses [19]. LLMs can excel in\n",
      "various domains within cybersecurity. Figure 1 highlights the\n",
      "top nine use cases and applications for LLMs in this field [20].\n",
      "1) Threat Detection and Analysis: LLMs can analyze\n",
      "vast network data in real-time to detect anomalies and\n",
      "potential threats. They can recognize patterns indicative\n",
      "of cyber attacks, such as malware, phishing attempts,\n",
      "and unusual network traffic [19].\n",
      "2) Security Automation: LLMs can facilitate the automa-\n",
      "\n",
      "Finally, possible future research directions for benefiting from\n",
      "LLMs in cyber security is discussed.\n",
      "I. I NTRODUCTION\n",
      "C\n",
      "Yber security is an essential realm that focuses primar-\n",
      "ily on safeguarding digital systems (computer systems,\n",
      "networks, and data) from any wrongdoings, including unautho-\n",
      "rized access or interruption. While technology has increasingly\n",
      "been integrating into every facet of life, the necessity of cyber\n",
      "security is growing as well. It encompasses a vast set of\n",
      "---------------------\n",
      "Given the context information above I want you to think step by step to answer the query in a crisp manner, in case you don't know the answer say 'I don't know!'.\n",
      "Query: How can we use LLM in cybersecurity?\n",
      "Answer: \n",
      "1. **Threat Detection and Analysis:** LLMs can analyze vast network data in real-time to detect anomalies and potential threats. They can recognize patterns indicative of cyber attacks, such as malware, phishing attempts, and unusual network traffic.\n",
      "2. **Security Automation:** LLMs can facilitate the automation of security tasks, such as vulnerability scanning, intrusion detection, and incident response.\n",
      "3. **Anomaly Detection:** LLMs can be used to detect anomalies in network traffic, system logs, and other data sources.\n",
      "4. **Program Repairing Binary IT Operations:** LLMs can assist in repairing binary code, which is essential for fixing security vulnerabilities and improving system performance.\n",
      "5. **Attack Simulation:** LLMs can be used to simulate attacks and test the effectiveness of security measures.\n",
      "6. **Threat Intelligence:** LLMs can be used to analyze and synthesize threat intelligence data to provide insights into emerging threats.\n",
      "7. **Vulnerability Detection:** LLMs can be used to detect vulnerabilities in software and systems.\n",
      "8. **Program Repairing Binary IT Operations:** LLMs can assist in repairing binary code, which is essential for fixing security vulnerabilities and improving system performance.\n",
      "9. **Anomaly Detection:** LLMs can be used to detect anomalies in network traffic, system logs, and other data sources.\n",
      "10. **Security Automation:** LLMs can facilitate the automation of security tasks, such as vulnerability scanning, intrusion detection, and incident response.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Purpose:\n",
    "* Implements a Retrieval-QA pipeline that:\n",
    "    * Fetches relevant documents using the retriever.\n",
    "    * Generates a response using the LLM chain.\n",
    "    * Optionally returns the source documents for reference.\n",
    "* Queries the system with a question about LLMs in cybersecurity as per the following steps:\n",
    "    * Retrieves relevant document chunks from Qdrant.\n",
    "    * Passes the context and question to the LLM.\n",
    "    * Outputs the generated answer."
   ],
   "id": "32be9c1563735d65"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
