{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Login to Hugging Face Hub",
   "id": "51400216e8edaabc"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-14T11:49:08.924306Z",
     "start_time": "2024-12-14T11:49:08.397266Z"
    }
   },
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7037e7c659f44380812b4908ceba70fe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Purpose: Logs into the Hugging Face Hub for access to pre-trained models.\n",
    "* Interaction: The user is prompted to authenticate using their Hugging Face credentials."
   ],
   "id": "96b32b2976888bbe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set Up the Device for Computation",
   "id": "5f7667b0008b3357"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check GPU Availability",
   "id": "b247066aad67fd4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:49:12.841837Z",
     "start_time": "2024-12-14T11:49:08.930312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "id": "53b6bb7e69c280f8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Device Setup",
   "id": "7e9adffdc65e5ac0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:49:12.931319Z",
     "start_time": "2024-12-14T11:49:12.924767Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.set_device(0)",
   "id": "ffc55456852c9c66",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This selects the GPU (if available) for running computations, optimizing performance for large models.",
   "id": "ef126cd1f2f4c042"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Required Libraries",
   "id": "58669e569003f41f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:49:22.070109Z",
     "start_time": "2024-12-14T11:49:13.423395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import LLMChain, StuffDocumentsChain\n",
    "from transformers import AutoTokenizer, pipeline"
   ],
   "id": "35f852a64c4f477d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* PyPDFDirectoryLoader: Loads and preprocesses PDF documents from a specified directory.\n",
    "* Qdrant: Stores and retrieves document embeddings for similarity-based search.\n",
    "* PromptTemplate: Defines structured templates for generating prompts with dynamic placeholders.\n",
    "* HuggingFacePipeline: Integrates Hugging Face's LLM pipelines into LangChain workflows.\n",
    "* HuggingFaceEmbeddings: Converts text into dense vector embeddings for retrieval tasks.\n",
    "* RetrievalQA: Chains document retrieval and LLM-based question answering.\n",
    "* RecursiveCharacterTextSplitter: Splits long texts into smaller, context-preserving chunks.\n",
    "* LLMChain: Links prompts with an LLM for structured response generation.\n",
    "* StuffDocumentsChain: Combines retrieved documents into a single context for LLM processing.\n",
    "* AutoTokenizer: Tokenizes text to prepare it for input into a Hugging Face model.\n",
    "* pipeline: Configures and executes text-generation tasks using Hugging Face models."
   ],
   "id": "386f69db48a501a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load PDF Documents",
   "id": "11115e5f75d0338b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:49:42.769445Z",
     "start_time": "2024-12-14T11:49:22.077124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loader = PyPDFDirectoryLoader(\"./pdf_directory\")\n",
    "docs = loader.load()\n",
    "print(len(docs))"
   ],
   "id": "f6b12232bfd5e3f2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Purpose: Reads all .pdf files from the specified directory and loads them as documents.\n",
    "* Output: The variable docs contains a list of document objects, and len(docs) prints the total number of documents loaded."
   ],
   "id": "55e56d98d833a6b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set Up the Embedding Model",
   "id": "5900f7c11a3369b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:49:52.051405Z",
     "start_time": "2024-12-14T11:49:42.778863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
   ],
   "id": "542588d66427ddde",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jucse\\AppData\\Local\\Temp\\ipykernel_24744\\807396544.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jucse\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Purpose: Initializes the HuggingFaceEmbeddings model to convert text into vector representations (embeddings).\n",
    "* Model Used: sentence-transformers/all-mpnet-base-v2, a widely-used transformer for embeddings."
   ],
   "id": "e0ef353342d6dd0b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Split Documents into Chunks",
   "id": "806799107917ad33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:49:52.131804Z",
     "start_time": "2024-12-14T11:49:52.059739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
    "all_splits = text_splitter.split_documents(docs)"
   ],
   "id": "6ed31c2fc81221fc",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Purpose: Splits large documents into smaller chunks of 500 characters for more effective indexing and retrieval.",
   "id": "bdc6a7cbefec4402"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create a Qdrant Vector Store",
   "id": "1052512b2a395058"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:50:32.247217Z",
     "start_time": "2024-12-14T11:49:52.139057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "qdrant_collection = Qdrant.from_documents(\n",
    "    all_splits,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"responses\",\n",
    ")"
   ],
   "id": "bc9a063cf4fd02fa",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Purpose: Stores the embedded document chunks in a Qdrant vector database, enabling similarity-based retrieval.",
   "id": "ae54072a304220b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configure a Retriever",
   "id": "9a12cfe73af2b467"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:50:32.259517Z",
     "start_time": "2024-12-14T11:50:32.256245Z"
    }
   },
   "cell_type": "code",
   "source": "qdrant_retriever = qdrant_collection.as_retriever()",
   "id": "5e779b9af1f80647",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Purpose: Configures the retriever to fetch documents from the Qdrant database based on a query's similarity to the stored embeddings.",
   "id": "eb6d32ba8e732618"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load and Configure the LLM from Hugging Face",
   "id": "94aa0c6c8970547c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:50:41.606253Z",
     "start_time": "2024-12-14T11:50:32.268598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"google/gemma-2-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cuda\",\n",
    "    max_new_tokens=512\n",
    ")"
   ],
   "id": "60d9b689bd5b71b7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c088ca257e044f0d909e48a4153fc7c9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Purpose: Initializes the Gemma-2 LLM for text generation using the Hugging Face pipeline.\n",
    "* Model Used: google/gemma-2-2b-it."
   ],
   "id": "3933c7089c5e1c8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Test the LLM with Chat-Style Prompts",
   "id": "6d3cfe57f15e0599"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:51:23.388581Z",
     "start_time": "2024-12-14T11:50:41.647966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Where is Milan?\"},\n",
    "]\n",
    "prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipeline(\n",
    "\tprompt,\n",
    "\tmax_new_tokens=256,\n",
    "\tadd_special_tokens=True,\n",
    "\tdo_sample=True,\n",
    "\ttemperature=0.7,\n",
    "\ttop_k=50,\n",
    "\ttop_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ],
   "id": "ffb22d56f4234ce2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'max_batch_size' argument of HybridCache is deprecated and will be removed in v4.46. Use the more precisely named 'batch_size' argument instead.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milan is a city located in **Northern Italy**. \n",
      "\n",
      "More specifically:\n",
      "\n",
      "* **Region:** Lombardy\n",
      "* **Country:** Italy\n",
      " \n",
      "It's the capital of the Lombardy region and is one of the major fashion, economic, and cultural centers of Italy. \n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Purpose: Tests the LLM with a chat-style prompt asking a geographical question.\n",
    "* Steps:\n",
    "    * Constructs a chat-based input format for the model.\n",
    "    * Generates a response using the LLM with specified parameters like temperature (controls randomness) and top-k/top-p sampling (controls diversity)."
   ],
   "id": "9b7b4f15b9527086"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define the Prompt Template",
   "id": "e63e29def0ebc068"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:51:23.471394Z",
     "start_time": "2024-12-14T11:51:23.466737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "qa_prompt_tmpl_str = (\n",
    "            \"Context information is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information above I want you to think step by step to answer the query in a crisp manner, in case you don't know the answer say 'I don't know!'.\\n\"\n",
    "            \"Query: {question}\\n\"\n",
    "            \"Answer: \"\n",
    "            )\n",
    "prompt_template = PromptTemplate.from_template(qa_prompt_tmpl_str)"
   ],
   "id": "c06fef6a722836ec",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Purpose: Defines a chain-of-thought style prompt template that:\n",
    "* Provides context.\n",
    "* Asks the model to answer step-by-step.\n",
    "* Prompts the model to explicitly state \"I don't know!\" if unsure."
   ],
   "id": "d2a285129e97a574"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create the LLM Chain and Combine Documents with the LLM Chain",
   "id": "ec6f0613aa2085bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:51:23.746153Z",
     "start_time": "2024-12-14T11:51:23.552401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gemma_llm = HuggingFacePipeline(\n",
    "    pipeline=pipeline,\n",
    "    model_kwargs={\"temperature\": 0.7},\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=gemma_llm, prompt=prompt_template)\n",
    "\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_variable_name=\"context\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(llm_chain.input_keys)  \n",
    "print(combine_documents_chain.input_keys)  "
   ],
   "id": "e01c8a5d8dce9759",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['context', 'question']\n",
      "['input_documents', 'question']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jucse\\AppData\\Local\\Temp\\ipykernel_24744\\3249647557.py:1: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  gemma_llm = HuggingFacePipeline(\n",
      "C:\\Users\\jucse\\AppData\\Local\\Temp\\ipykernel_24744\\3249647557.py:6: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=gemma_llm, prompt=prompt_template)\n",
      "C:\\Users\\jucse\\AppData\\Local\\Temp\\ipykernel_24744\\3249647557.py:8: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain/\n",
      "  combine_documents_chain = StuffDocumentsChain(\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Purpose:\n",
    "* Connects the Gemma LLM with the defined prompt template to form a processing pipeline.\n",
    "* Processes the retrieved documents to form a cohesive input (context) for the LLM."
   ],
   "id": "fe2c13a7c92fe1e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set Up the Retrieval-QA Pipeline and Query the Retrieval-QA System",
   "id": "89116f8916451628"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:55:21.170282Z",
     "start_time": "2024-12-14T11:51:23.757162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "qa = RetrievalQA(\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    retriever=qdrant_retriever,\n",
    "    verbose=True,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "print(qa.input_keys)\n",
    "\n",
    "query_str = \"How can we use LLM in cybersecurity?\"\n",
    "response = qa.invoke(query_str)\n",
    "\n",
    "print(response[\"result\"])\n"
   ],
   "id": "32389c2a1673da2c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jucse\\AppData\\Local\\Temp\\ipykernel_24744\\1815635984.py:1: LangChainDeprecationWarning: This class is deprecated. Use the `create_retrieval_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/retrieval_qa/\n",
      "  qa = RetrievalQA(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['query']\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQA chain...\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new StuffDocumentsChain chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Context information is below.\n",
      "---------------------\n",
      "Table 1: The main cybersecurity tasks and applications where LLMs have been utility.\n",
      "Vulnerability\n",
      "Detection\n",
      "(In)secure\n",
      "Code\n",
      "Generation\n",
      "Program\n",
      "Repairing Binary IT\n",
      "Operations\n",
      "Threat\n",
      "Intelligence\n",
      "Anomaly\n",
      "Detection\n",
      "LLM\n",
      "Assisted\n",
      "Attack\n",
      "Others\n",
      "RQ1 âœ“ âœ“ âœ“ âœ“ âœ“ - - - âœ“\n",
      "RQ2 âœ“ âœ“ âœ“ - - âœ“ âœ“ âœ“ âœ“\n",
      "RQ3 - - - - âœ“ - âœ“ âœ“ -\n",
      "Despite initial efforts of LLMs in cybersecurity, the field faces several challenges [17, 31]. First, many studies rely on\n",
      "\n",
      "importance of LLMs in the field of cybersecurity, although the researches of LLMsâ€™ application in their fields\n",
      "are less.\n",
      "3 RQ1: How to construct cybersecurity-oriented domain LLMs?\n",
      "The cybersecurity domain is facing escalating threats, demanding intelligent and efficient solutions to counter sophisti-\n",
      "cated and evolving attacks [37, 38, 39]. LLMs offers novel opportunities for the cybersecurity community [ 18, 19].\n",
      "\n",
      "decisions and prioritize responses [19]. LLMs can excel in\n",
      "various domains within cybersecurity. Figure 1 highlights the\n",
      "top nine use cases and applications for LLMs in this field [20].\n",
      "1) Threat Detection and Analysis: LLMs can analyze\n",
      "vast network data in real-time to detect anomalies and\n",
      "potential threats. They can recognize patterns indicative\n",
      "of cyber attacks, such as malware, phishing attempts,\n",
      "and unusual network traffic [19].\n",
      "2) Security Automation: LLMs can facilitate the automa-\n",
      "\n",
      "Finally, possible future research directions for benefiting from\n",
      "LLMs in cyber security is discussed.\n",
      "I. I NTRODUCTION\n",
      "C\n",
      "Yber security is an essential realm that focuses primar-\n",
      "ily on safeguarding digital systems (computer systems,\n",
      "networks, and data) from any wrongdoings, including unautho-\n",
      "rized access or interruption. While technology has increasingly\n",
      "been integrating into every facet of life, the necessity of cyber\n",
      "security is growing as well. It encompasses a vast set of\n",
      "---------------------\n",
      "Given the context information above I want you to think step by step to answer the query in a crisp manner, in case you don't know the answer say 'I don't know!'.\n",
      "Query: How can we use LLM in cybersecurity?\n",
      "Answer: \n",
      "1. **Threat Detection and Analysis:** LLMs can analyze vast network data in real-time to detect anomalies and potential threats. They can recognize patterns indicative of cyber attacks, such as malware, phishing attempts, and unusual network traffic.\n",
      "2. **Security Automation:** LLMs can facilitate the automation of security tasks, such as vulnerability scanning, intrusion detection, and incident response.\n",
      "3. **Anomaly Detection:** LLMs can be used to detect anomalies in network traffic, system logs, and other data sources.\n",
      "4. **Program Repairing Binary IT Operations:** LLMs can assist in repairing binary code, which is essential for fixing security vulnerabilities and improving system performance.\n",
      "5. **Attack Simulation:** LLMs can be used to simulate attacks and test the effectiveness of security measures.\n",
      "6. **Threat Intelligence:** LLMs can be used to analyze and synthesize threat intelligence data to provide insights into emerging threats.\n",
      "7. **Vulnerability Detection:** LLMs can be used to detect vulnerabilities in software and systems.\n",
      "8. **Program Repairing Binary IT Operations:** LLMs can assist in repairing binary code, which is essential for fixing security vulnerabilities and improving system performance.\n",
      "9. **Anomaly Detection:** LLMs can be used to detect anomalies in network traffic, system logs, and other data sources.\n",
      "10. **Security Automation:** LLMs can facilitate the automation of security tasks, such as vulnerability scanning, intrusion detection, and incident response.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Purpose:\n",
    "* Implements a Retrieval-QA pipeline that:\n",
    "    * Fetches relevant documents using the retriever.\n",
    "    * Generates a response using the LLM chain.\n",
    "    * Optionally returns the source documents for reference.\n",
    "* Queries the system with a question about LLMs in cybersecurity as per the following steps:\n",
    "    * Retrieves relevant document chunks from Qdrant.\n",
    "    * Passes the context and question to the LLM.\n",
    "    * Outputs the generated answer."
   ],
   "id": "32be9c1563735d65"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
