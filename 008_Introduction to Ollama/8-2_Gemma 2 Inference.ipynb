{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pre-requisite\n",
    "**Setup Ollama**<br>\n",
    "For setting up ollama in your local environment, click [here](https://github.com/ollama/ollama).\n",
    "\n",
    "**Pull Model**<br>\n",
    "Pull the required model running the following command:\n",
    "```bash\n",
    "ollama pull gemma2\n",
    "```\n",
    "To use more variants of the gemma model check [here](https://ollama.com/library).\n",
    "\n",
    "**Install python library**<br>\n",
    "Install the python library for ollama\n",
    "```bash\n",
    "pip install ollama\n",
    "```\n",
    "To learn more, click [here](https://github.com/ollama/ollama-python)."
   ],
   "id": "4f3b68e276a59b3e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Libraries",
   "id": "cf314ea468351fa8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-11T11:08:19.535821Z",
     "start_time": "2024-10-11T11:08:19.166978Z"
    }
   },
   "source": [
    "import ollama\n",
    "from IPython.display import display, Markdown\n",
    "from numpy import random"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* ollama: This library allows you to interact with models hosted by Ollama (e.g., the llama3.2 model). It supports sending messages and receiving responses from the model.\n",
    "* IPython.display: The display function and Markdown module are used to format and display the modelâ€™s response as formatted markdown in a Jupyter/IPython environment.\n",
    "* numpy.random: This is used for generating random numbers and controlling randomness in code execution."
   ],
   "id": "123adc865b4aa62a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setting a Random Seed",
   "id": "176ad83400b3885c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T11:08:19.551805Z",
     "start_time": "2024-10-11T11:08:19.539821Z"
    }
   },
   "cell_type": "code",
   "source": "random.seed(42)",
   "id": "d0317ddeae3acc17",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This sets a seed for reproducibility in any random number generation used within the code. Though not directly related to the model, it ensures the random processes (if any) are repeatable.",
   "id": "e3851e89c27f67f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Making the First Request to Ollama's Gemma2 Model",
   "id": "7e3402598aa9b348"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T11:10:13.862516Z",
     "start_time": "2024-10-11T11:08:19.616294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response1 = ollama.chat(model='gemma2', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Tell me about Machine Learning.',\n",
    "  },\n",
    "])\n",
    "display(Markdown(response1['message']['content']))"
   ],
   "id": "3e61fb97deb917dc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "##  Machine Learning: Teaching Computers to Learn \n\nMachine learning (ML) is a fascinating field of artificial intelligence (AI) where computers learn from data without explicit programming. Instead of relying on rigid rules, ML algorithms identify patterns and insights within data, allowing them to make predictions or decisions. \n\nThink of it like this: instead of giving a computer step-by-step instructions for every scenario, we feed it massive amounts of data and let it figure out the rules itself.\n\n**Here's a breakdown:**\n\n* **Data is King:** ML algorithms thrive on data. The more relevant and high-quality data you provide, the better the algorithm learns.\n* **Types of Learning:**\n\n    * **Supervised learning:** The algorithm is trained on labeled data (input-output pairs), learning to map inputs to desired outputs. Examples: image classification, spam detection.\n    * **Unsupervised learning:** The algorithm explores unlabeled data, identifying patterns and structures within it. Examples: customer segmentation, anomaly detection.\n    * **Reinforcement learning:** The algorithm learns through trial and error, receiving rewards for correct actions and penalties for incorrect ones. Examples: game playing, robotics control.\n\n* **Applications are Everywhere:** ML is transforming countless industries:\n\n    * **Healthcare:** Diagnosing diseases, predicting patient outcomes, drug discovery.\n    * **Finance:** Fraud detection, credit scoring, personalized financial advice.\n    * **Marketing:** Targeted advertising, customer segmentation, sentiment analysis.\n    * **Technology:** Recommender systems, natural language processing, image recognition.\n\n**Challenges and Considerations:**\n\n* **Bias in data:** ML algorithms can inherit biases present in the training data, leading to unfair or discriminatory outcomes.\n* **Data privacy:** Handling sensitive data responsibly is crucial.\n* **Explainability:** Understanding how an ML model arrives at its decisions can be challenging, raising ethical concerns.\n\n\n**The Future of Machine Learning:**\n\nML is rapidly evolving, with advancements in areas like deep learning and explainable AI pushing the boundaries of what's possible. As data becomes even more abundant and powerful algorithms emerge, we can expect to see even more transformative applications of ML across all aspects of our lives."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "**Explanation of the Code**<br>\n",
    "* The Ollama chat API is used to interact with the gemma2 model.\n",
    "* The prompt given by the user is 'Tell me about Machine Learning.'.\n",
    "* The request is formatted as a message, where the role is 'user' and the content is the prompt.\n",
    "* The ollama.chat() function sends this prompt to the gemma2 model.\n",
    "* The response is fetched and then displayed using the Markdown() function from IPython, making it appear as nicely formatted text in the notebook or IPython interface."
   ],
   "id": "75d35f73a81f2a3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Making the Second Request to Ollama Gemma2 Model",
   "id": "28edfeba82120b2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T11:12:34.636799Z",
     "start_time": "2024-10-11T11:10:13.880565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response2 = ollama.chat(\n",
    "  model='gemma2', \n",
    "  messages=[\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': 'What is Large Language Model?',\n",
    "    },],\n",
    "  stream=True\n",
    ")\n",
    "for chunk in response2:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ],
   "id": "1d6743792f1ad41b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Large Language Model (LLM) is a type of artificial intelligence (AI) that excels at understanding and generating human language. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "**Key Characteristics:**\n",
      "\n",
      "* **Massive Scale:** LLMs are trained on colossal datasets of text and code, containing billions or even trillions of words. This vast amount of data allows them to learn complex patterns and relationships within language.\n",
      "* **Deep Learning:** They utilize deep learning algorithms, specifically a type of neural network called a transformer, which enables them to process and understand the context of words in a sentence effectively.\n",
      "\n",
      "**Capabilities:**\n",
      "\n",
      "* **Text Generation:** LLMs can generate human-quality text in various styles and tones. This includes writing stories, poems, articles, summaries, and even code.\n",
      "* **Language Translation:** They can accurately translate text from one language to another.\n",
      "* **Question Answering:** LLMs can answer questions based on a given context or a vast store of knowledge they have acquired during training.\n",
      "* **Conversation:** They can engage in natural-sounding conversations, responding to prompts and carrying on dialogue.\n",
      "* **Summarization and Analysis:** LLMs can condense large amounts of text into concise summaries and identify key themes or sentiments.\n",
      "\n",
      "**Examples:**\n",
      "\n",
      "Some well-known LLMs include:\n",
      "\n",
      "* **GPT-3 (Generative Pre-trained Transformer 3)** by OpenAI\n",
      "* **LaMDA (Language Model for Dialogue Applications)** by Google\n",
      "* **BERT (Bidirectional Encoder Representations from Transformers)** by Google\n",
      "* **BLOOM (BigScience Large Open-science Open-access Multilingual Language Model)**\n",
      "\n",
      "**Impact:**\n",
      "\n",
      "LLMs are revolutionizing many fields, including:\n",
      "\n",
      "* **Customer Service:** Chatbots powered by LLMs can provide quick and efficient customer support.\n",
      "* **Education:** Personalized learning experiences and automated essay grading.\n",
      "* **Content Creation:** Assisting writers in generating ideas, overcoming writer's block, and improving writing quality.\n",
      "* **Research:** Accelerating scientific discovery through text analysis and knowledge extraction.\n",
      "\n",
      "**Ethical Considerations:**\n",
      "\n",
      "The power of LLMs also raises ethical concerns, such as:\n",
      "\n",
      "* **Bias and Fairness:** LLMs can perpetuate biases present in the training data, leading to unfair or discriminatory outputs.\n",
      "* **Misinformation and Manipulation:** LLMs can be used to generate convincing fake news and propaganda.\n",
      "* **Job Displacement:** Automation powered by LLMs may lead to job losses in certain sectors.\n",
      "\n",
      "\n",
      "It's important to use LLMs responsibly and address these ethical challenges to ensure they benefit society as a whole."
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* A second request is sent to the gemma2 model with a different prompt: 'What is Large Language Model?'.\n",
    "* This time, streaming is enabled by setting stream=True, which allows receiving the response in chunks.\n",
    "* The server responds with pieces of the generated text as soon as they are available, rather than waiting for the full response to be generated.\n",
    "* A for loop iterates over each chunk of the response as it's streamed back from the model.\n",
    "* The print() function outputs the chunk's content without adding new lines (end=''), so the output appears as continuous text.\n",
    "* The flush=True ensures that each chunk is displayed immediately without delay, giving a real-time effect to the response output."
   ],
   "id": "ea3ba2d67c3d58e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "* The code interacts with the gemma2 model using Ollama's API. It sends two requests: one for a full response and another with real-time streaming of the output.\n",
    "* The responses are displayed using IPython's Markdown for nicely formatted output, and the streaming response is shown in real-time using a loop that processes chunks of data."
   ],
   "id": "5bb8e53f7ff50b58"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
