{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPvROa9IE+eWp5mtbniNBf8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Install Required Library**"],"metadata":{"id":"YXG7DKbqwR2U"}},{"cell_type":"code","source":["!pip install pyspellchecker"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2VTBr0y0phYe","executionInfo":{"status":"ok","timestamp":1720582751265,"user_tz":-360,"elapsed":13703,"user":{"displayName":"Mahiyat Tanzim","userId":"03004994999431640800"}},"outputId":"e10daaee-03a8-4259-d386-e800ca1b1d7f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.10/dist-packages (0.8.1)\n"]}]},{"cell_type":"markdown","source":["!pip install pyspellchecker: Installs the pyspellchecker library, which is used for spelling correction."],"metadata":{"id":"NaqE6_ydwWlE"}},{"cell_type":"markdown","source":["**Import Required Libraries**"],"metadata":{"id":"pUbxTPTLwaEM"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Md4Y_OSUmbCx","executionInfo":{"status":"ok","timestamp":1720582752600,"user_tz":-360,"elapsed":1337,"user":{"displayName":"Mahiyat Tanzim","userId":"03004994999431640800"}},"outputId":"6c147941-bddb-4e56-8255-6ed674802509"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["import string\n","import nltk\n","import re\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from spellchecker import SpellChecker\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')"]},{"cell_type":"markdown","source":["\n","\n","*   import ...: Imports necessary libraries and modules.\n","    *   string: Used for string operations.\n","    *   nltk and its submodules: Used for natural language processing tasks.\n","    *   re: Used for regular expressions.\n","    *   SpellChecker: Used for spelling correction.\n","*   nltk.download(...): Downloads required NLTK data files for stopwords, tokenization, and lemmatization.\n","\n"],"metadata":{"id":"IZK3q8GHwfpD"}},{"cell_type":"markdown","source":["**Lowercase Conversion**"],"metadata":{"id":"TAiDaxLow_pi"}},{"cell_type":"code","source":["def lowercase(text):\n","  text=text.lower()\n","  print(\"Lowercase: \", text)\n","  return text"],"metadata":{"id":"kVjbmodtm1Oi","executionInfo":{"status":"ok","timestamp":1720582752600,"user_tz":-360,"elapsed":4,"user":{"displayName":"Mahiyat Tanzim","userId":"03004994999431640800"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["**Remove Punctuation**"],"metadata":{"id":"ibV0upt0xEk2"}},{"cell_type":"code","source":["def remove_punctuation(text):\n","  translator = str.maketrans('', '', string.punctuation)\n","  text = text.translate(translator)\n","  print(\"Punctuation Removed: \", text)\n","  return text"],"metadata":{"id":"9v5nBA8mm9aq","executionInfo":{"status":"ok","timestamp":1720582752600,"user_tz":-360,"elapsed":4,"user":{"displayName":"Mahiyat Tanzim","userId":"03004994999431640800"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["**Remove Special Characters**"],"metadata":{"id":"KROmROVsxKhq"}},{"cell_type":"code","source":["def remove_special_chars(text):\n","  text = re.sub(r'[^\\w\\s]','',text)\n","  print(\"Special Characters Removed: \", text)\n","  return text"],"metadata":{"id":"RNpfr98anKjG","executionInfo":{"status":"ok","timestamp":1720582752600,"user_tz":-360,"elapsed":4,"user":{"displayName":"Mahiyat Tanzim","userId":"03004994999431640800"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["**Remove Stopwords**"],"metadata":{"id":"JdqnmnyoxNv9"}},{"cell_type":"code","source":["def remove_stopwords(text):\n","  stop_words = set(stopwords.words('english'))\n","  tokens = nltk.word_tokenize(text)\n","  filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n","  filtered_text = ' '.join(filtered_tokens)\n","  print(\"Stopwords Removed: \", filtered_text)\n","  return filtered_text"],"metadata":{"id":"WywMeQ01nZ74","executionInfo":{"status":"ok","timestamp":1720582752600,"user_tz":-360,"elapsed":4,"user":{"displayName":"Mahiyat Tanzim","userId":"03004994999431640800"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["**Standardize Text (Lemmatization)**"],"metadata":{"id":"2oyWgbMexTJk"}},{"cell_type":"code","source":["def standardize_text(text):\n","  tokens = nltk.word_tokenize(text)\n","  lemmatizer = WordNetLemmatizer()\n","  standardized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","  standardized_text = ' '.join(standardized_tokens)\n","  print(\"Standardized Text: \", standardized_text)\n","  return standardized_text"],"metadata":{"id":"SXFhSPpyn93l","executionInfo":{"status":"ok","timestamp":1720582752600,"user_tz":-360,"elapsed":3,"user":{"displayName":"Mahiyat Tanzim","userId":"03004994999431640800"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["**Correct Spelling**"],"metadata":{"id":"TGY98J1uxWw8"}},{"cell_type":"code","source":["def correct_spelling(text):\n","  corrected_text = \"\"\n","  spell = SpellChecker()\n","  corrected_text = ' '.join([spell.correction(word) or word for word in text.split()])\n","  print(\"Corrected Text: \", corrected_text)\n","  return corrected_text"],"metadata":{"id":"WGSdxnfmoSZX","executionInfo":{"status":"ok","timestamp":1720582752600,"user_tz":-360,"elapsed":3,"user":{"displayName":"Mahiyat Tanzim","userId":"03004994999431640800"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["**Define Text Cleaning Pipeline**"],"metadata":{"id":"B07C3Jwoxa3d"}},{"cell_type":"code","source":["def clean_text(text):\n","  cleaned_text = lowercase(text)\n","  cleaned_text = remove_punctuation(cleaned_text)\n","  cleaned_text = remove_special_chars(cleaned_text)\n","  cleaned_text = remove_stopwords(cleaned_text)\n","  cleaned_text = standardize_text(cleaned_text)\n","  cleaned_text = correct_spelling(cleaned_text)\n","  return cleaned_text"],"metadata":{"id":"hK-Q0NyVoiIA","executionInfo":{"status":"ok","timestamp":1720582752600,"user_tz":-360,"elapsed":3,"user":{"displayName":"Mahiyat Tanzim","userId":"03004994999431640800"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["**Apply Cleaning Pipeline to Sample Texts**"],"metadata":{"id":"RPESGeTcxfjk"}},{"cell_type":"code","source":["text1 = \"Hello World! This is an example of text cleaning using Python.\"\n","cleaned_text1 = clean_text(text1)\n","seperator = '*'*100\n","print(seperator)\n","print(\"Cleaned text: \",cleaned_text1)\n","\n","print(\"\\n\")\n","\n","text2 = \"Hi! I'm Mahiyat and I'm a CSE undergraduate. I love working with #data and #NLPs. I have a litle experience in this fielld.\"\n","cleaned_text2 = clean_text(text2)\n","print(seperator)\n","print(\"Cleaned text: \",cleaned_text2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AYMZrE86ov-M","executionInfo":{"status":"ok","timestamp":1720582758245,"user_tz":-360,"elapsed":5648,"user":{"displayName":"Mahiyat Tanzim","userId":"03004994999431640800"}},"outputId":"f4708b03-9fe5-4b29-e3ab-9bb7acd9cd18"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Lowercase:  hello world! this is an example of text cleaning using python.\n","Punctuation Removed:  hello world this is an example of text cleaning using python\n","Special Characters Removed:  hello world this is an example of text cleaning using python\n","Stopwords Removed:  hello world example text cleaning using python\n","Standardized Text:  hello world example text cleaning using python\n","Corrected Text:  hello world example text cleaning using python\n","****************************************************************************************************\n","Cleaned text:  hello world example text cleaning using python\n","\n","\n","Lowercase:  hi! i'm mahiyat and i'm a cse undergraduate. i love working with #data and #nlps. i have a litle experience in this fielld.\n","Punctuation Removed:  hi im mahiyat and im a cse undergraduate i love working with data and nlps i have a litle experience in this fielld\n","Special Characters Removed:  hi im mahiyat and im a cse undergraduate i love working with data and nlps i have a litle experience in this fielld\n","Stopwords Removed:  hi im mahiyat im cse undergraduate love working data nlps litle experience fielld\n","Standardized Text:  hi im mahiyat im cse undergraduate love working data nlp litle experience fielld\n","Corrected Text:  hi i mahiyat i use undergraduate love working data nap little experience field\n","****************************************************************************************************\n","Cleaned text:  hi i mahiyat i use undergraduate love working data nap little experience field\n"]}]},{"cell_type":"markdown","source":["\n","\n","*   Defines two sample texts (text1 and text2).\n","*   Cleans each text using the clean_text function.\n","*   Prints the cleaned text along with a separator for clarity.\n","\n","\n"],"metadata":{"id":"WFZ4zx0BxkXI"}},{"cell_type":"markdown","source":["**Summary**<br>\n","This notebook provides a comprehensive pipeline for cleaning text data, including converting to lowercase, removing punctuation and special characters, removing stopwords, lemmatizing, and correcting spelling. The cleaning functions are applied in sequence to the sample texts, and the results are printed to demonstrate the effect of each step."],"metadata":{"id":"MAofXXkHxtsR"}}]}