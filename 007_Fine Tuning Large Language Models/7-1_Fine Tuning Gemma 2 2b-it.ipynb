{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task\n",
    "To fine-tune a pre-trained large language model (gemma2 2b it) for multi-class classification task using Quantized Low-Ranking Adaptation (QLoRA)."
   ],
   "id": "17b6397bd8efcb12"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Environment Setup",
   "id": "e1db2a03292c607a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Login to Hugging Face Hub",
   "id": "8bb706e116727346"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ],
   "id": "8e992b19a7030634",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "login() is used to authenticate with Hugging Face to load the model from their hub.",
   "id": "6a5f766e114035e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check GPU Availability",
   "id": "719a1fde8c8a9035"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:31:38.817654Z",
     "start_time": "2024-10-17T06:31:36.302340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "id": "978b9e7e30c74c19",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Device Setup",
   "id": "b2dabbd8ce79cfdf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:31:40.152044Z",
     "start_time": "2024-10-17T06:31:40.145875Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.set_device(0)",
   "id": "f2c7d308f35b7636",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This selects the GPU (if available) for running computations, optimizing performance for large models.",
   "id": "f567c65b8b6c2bd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Pretrained Model and Tokenizer",
   "id": "a2e20b43b4aff6a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define ID and Label Mapping",
   "id": "9e6cf5fc5e28108c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:31:41.354022Z",
     "start_time": "2024-10-17T06:31:41.350743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "id2label = {0: \"negative\", 1: \"positive\", 2: \"neutral\"}\n",
    "label2id = {\"negative\": 0, \"positive\": 1, \"neutral\": 2}"
   ],
   "id": "75983c8909e61076",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define Model and Tokenizer",
   "id": "2317acd7fce3a834"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:31:52.057261Z",
     "start_time": "2024-10-17T06:31:42.515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "\n",
    "bnbConfig = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    device_map = \"auto\",\n",
    "    quantization_config=bnbConfig\n",
    ")"
   ],
   "id": "eafe83099e4bbad0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c048dd0760344140afcdd6ca573aa50c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at google/gemma-2-2b-it and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Define the model name to be used.\n",
    "* Configure BitsAndBytesConfig Parameters: This configuration is used to quantize the model using the bitsandbytes library. Quantization helps in reducing the memory footprint and computation required for model training and inference, which is essential when dealing with large models like Gemma-2-2b.\n",
    "    * load_in_4bit: Enables 4-bit precision to reduce memory usage and speed up computations.\n",
    "    * bnb_4bit_quant_type: Specifies the quantization type (e.g., nf4), which impacts the trade-off between accuracy and efficiency.\n",
    "    * bnb_4bit_use_double_quant: Adds a second layer of quantization to retain some precision despite low-bit encoding.\n",
    "    * bnb_4bit_compute_dtype: Sets the data type for computations (bfloat16 is commonly used in GPUs as it provides efficient precision and computation speed).\n",
    "* Initialize tokenizer for the defined model\n",
    "* Load the model:\n",
    "    * model_name: Name of the defined model.\n",
    "    * num_labels=3: Use for multi-class classification (e.g., positive, neutral, negative).\n",
    "    * id2label=id2label: A dictionary that maps output class indices (ids) to human-readable labels. This is used to convert model predictions into interpretable labels.\n",
    "    * label2id=label2id: A dictionary that maps human-readable labels to their corresponding indices (ids). This is used during model fine-tuning to convert the dataset's labels into the correct format for the model.\n",
    "    * device_map=\"auto\": Automatically maps model layers to available devices.\n",
    "    * quantization_config=bnbConfig: This specifies the quantization settings (e.g., 4-bit quantization) to reduce model size and improve efficiency."
   ],
   "id": "a1268bc4b4659d2c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check Where Model is Loaded",
   "id": "c90b9826751c34a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:31:57.140971Z",
     "start_time": "2024-10-17T06:31:57.137075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loaded_device = next(model.parameters()).device\n",
    "print(f\"The model is loaded on: {loaded_device}\")"
   ],
   "id": "b345aaa49d5d1fae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is loaded on: cuda:0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sentiment Inference Before Fine-Tuning",
   "id": "53d87b63fb24b469"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inference 1",
   "id": "fe5d2c0fe1dbebdb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:32:04.023178Z",
     "start_time": "2024-10-17T06:31:58.557257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"@united I loved the flight!\"\n",
    "    \n",
    "inputs = tokenizer(input_text, return_tensors='pt', truncation=True).to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "predicted_logits = outputs.logits\n",
    "predicted_class_id = torch.argmax(predicted_logits, dim=1).item()\n",
    "predicted_label = id2label[predicted_class_id]\n",
    "\n",
    "print(predicted_class_id)\n",
    "print(\"Predicted Sentiment:\",predicted_label)"
   ],
   "id": "debae05789768709",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Predicted Sentiment: neutral\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inference 2",
   "id": "8842a813a06532af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:32:08.155634Z",
     "start_time": "2024-10-17T06:32:07.697810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"Need to improve your airline service\"\n",
    "    \n",
    "inputs = tokenizer(input_text, return_tensors='pt', truncation=True).to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "predicted_logits = outputs.logits\n",
    "predicted_class_id = torch.argmax(predicted_logits, dim=1).item()\n",
    "predicted_label = id2label[predicted_class_id]\n",
    "\n",
    "print(predicted_class_id)\n",
    "print(\"Predicted Sentiment:\",predicted_label)"
   ],
   "id": "5387012be71aee1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Predicted Sentiment: neutral\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The steps are:\n",
    "* Input Text: The input text is tokenized and sent to the GPU for processing.\n",
    "* Model Inference: The model outputs logits for each sentiment class (positive, negative, neutral).\n",
    "* Prediction: The predicted sentiment class is determined using the argmax function on the logits."
   ],
   "id": "bfe8dc22a69dc1fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preparation for Fine-Tuning",
   "id": "60db08de10482957"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Dataset",
   "id": "1cc3cbafe260db65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:32:11.052926Z",
     "start_time": "2024-10-17T06:32:10.128402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Datasets/Airline-Sentiment-2-w-AA.csv', encoding='ISO-8859-1')\n",
    "\n",
    "filtered_df = df[['text', 'airline_sentiment']]\n",
    "\n",
    "dataset = Dataset.from_pandas(filtered_df)\n",
    "\n",
    "shuffled_dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "shuffled_dataset"
   ],
   "id": "5213711002193ff1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'airline_sentiment'],\n",
       "    num_rows: 14640\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The airline sentiment dataset is loaded, filtered, shuffled, and converted into a Hugging Face dataset.",
   "id": "a9cfddc1d0dfc2f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train, Validation, and Test Splits",
   "id": "1c382b54935301c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Split Dataset",
   "id": "dae2eb879891d334"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:32:13.810686Z",
     "start_time": "2024-10-17T06:32:13.796532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_val_test_split = shuffled_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_val_dataset = train_val_test_split['train']\n",
    "test_dataset = train_val_test_split['test']\n",
    "\n",
    "train_val_split = train_val_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "train_dataset = train_val_split['train']\n",
    "validation_dataset = train_val_split['test']"
   ],
   "id": "451a3ea2710758b7",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The data is split into training, validation, and test sets (72-8-20 split).",
   "id": "bf17b4dd40aee9b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check Dataset Size",
   "id": "e224c9af5afd6ddf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:32:15.745680Z",
     "start_time": "2024-10-17T06:32:15.741530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Validation size: {len(validation_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")"
   ],
   "id": "68aa473e3bc7c487",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 10540\n",
      "Validation size: 1172\n",
      "Test size: 2928\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fine-Tuning the Model with QLoRA",
   "id": "c5bf4c390cfce0db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define Tokenization Function",
   "id": "4c097fdcf529363c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:32:17.506746Z",
     "start_time": "2024-10-17T06:32:17.502622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_function(examples):\n",
    "    tokenized_output = tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "    tokenized_output['labels'] = [label2id[label] for label in examples['airline_sentiment']]\n",
    "    return tokenized_output"
   ],
   "id": "791b5d89d83e0138",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenization of Dataset",
   "id": "b39348dcbcf9559f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:32:20.773871Z",
     "start_time": "2024-10-17T06:32:19.417824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_validation_dataset = validation_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ],
   "id": "af500fbb89f87453",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10540 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7893aa3bf3fe45ba9b74061e66667a1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1172 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "63435c40067646008333061a9540f098"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2928 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "677c3128d38c41d2b24a61ba3820ee3d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The text and labels are tokenized and converted into numerical representations using the tokenizer.",
   "id": "c5ab3aa1e18fbf63"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### QLoRA Preparation:",
   "id": "ca4a2dfb380d98f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:32:22.254269Z",
     "start_time": "2024-10-17T06:32:22.250130Z"
    }
   },
   "cell_type": "code",
   "source": "model.gradient_checkpointing_enable()",
   "id": "d659b16e5945fda2",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:32:24.488334Z",
     "start_time": "2024-10-17T06:32:24.200973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "prepare_model_for_kbit_training(model)"
   ],
   "id": "d518fe1dda58a174",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2ForSequenceClassification(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "  )\n",
       "  (score): Linear(in_features=2304, out_features=3, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The model is prepared for fine-tuning using LoRA (Low-Rank Adaptation) with 4-bit quantization.",
   "id": "e22593fd8d2bac4e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Identify LoRA Modules",
   "id": "470439c28cb97ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:32:26.473580Z",
     "start_time": "2024-10-17T06:32:26.468371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit  \n",
    "\n",
    "    lora_module_names = set()\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "        if 'lm_head' in lora_module_names: \n",
    "            lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_linear_names(model)"
   ],
   "id": "b8bf837351bc6f67",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:32:30.836539Z",
     "start_time": "2024-10-17T06:32:30.832957Z"
    }
   },
   "cell_type": "code",
   "source": "print(modules)",
   "id": "d71f9bb422568b3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o_proj', 'down_proj', 'gate_proj', 'q_proj', 'up_proj', 'k_proj', 'v_proj']\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code defines a function that utilizes bitsandbytes.nn modules to identify all learnable linear layers within the quantized GEMMA-2b model. Candidates layers for the LoRA adapter are printed.",
   "id": "f7f3e0f02d67578a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LoRA Configuration",
   "id": "6e88716e8bbd0d65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:32:33.110715Z",
     "start_time": "2024-10-17T06:32:32.592542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ],
   "id": "a01e28ce2825bf7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 10,390,272 || all params: 2,624,739,072 || trainable%: 0.3959\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This configuration defines the LoRA (Low-Rank Adaptation) setup. LoRA helps in fine-tuning large models by freezing most of the model's layers and only training low-rank weight matrices, significantly reducing the number of trainable parameters.\n",
    "* r: Determines the rank of the low-rank decomposition. Smaller ranks reduce the number of parameters but may lead to lower accuracy.\n",
    "* lora_alpha: Controls the scaling factor for LoRA layers, influencing how much impact these low-rank weights have.\n",
    "* target_modules: Specifies which layers of the model will use LoRA for training (identified via find_linear_names()).\n",
    "* lora_dropout: Dropout rate for LoRA layers to prevent overfitting.\n",
    "* bias: Controls whether to use bias in the LoRA weights; \"none\" means no additional bias terms.\n",
    "* task_type: Specifies the type of task (e.g., sequence classification)."
   ],
   "id": "4796f4a1a349a8ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define Data Collator",
   "id": "bcf0b8c056f5534d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:32:35.729306Z",
     "start_time": "2024-10-17T06:32:35.093443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer\n",
    ")"
   ],
   "id": "2c97a436d91615e2",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The DataCollatorWithPadding class from Transformers helps prepare batches of data for training. It handles padding sequences to a common length and creates attention masks.",
   "id": "9c0a89a15119442f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training and Evaluation Setup",
   "id": "441c5cae92fe201c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define Training Arguments",
   "id": "1fae111d195aed0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:32:40.506400Z",
     "start_time": "2024-10-17T06:32:37.864532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=3,                \n",
    "    report_to=\"none\",\n",
    "    metric_for_best_model='eval_loss',\n",
    ")"
   ],
   "id": "ffcd85da51ebed22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jucse\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "These parameters define how the model is trained, optimized, and validated during fine-tuning.\n",
    "* output_dir: Where all the outputs from training, including model checkpoints, logs, and results, are saved.\n",
    "* learning_rate: Controls how much to adjust the model's weights with each step. A lower value (e.g., 2e-5) helps in fine-tuning by making smaller, controlled updates.\n",
    "* per_device_train_batch_size: Number of samples processed in parallel during training for each GPU. A smaller batch size is used for memory efficiency.\n",
    "* per_device_eval_batch_size: Number of samples processed in parallel during evaluation.\n",
    "* weight_decay: Regularization to prevent the model from overfitting by shrinking large weights.\n",
    "* eval_strategy: Defines when to run evaluation during training (e.g., after each epoch).\n",
    "* save_strategy: When to save checkpoints of the model (here, after each epoch).\n",
    "* load_best_model_at_end: Ensures that the model with the lowest evaluation loss is used after training.\n",
    "* num_train_epochs: Specifies how many full passes over the dataset the model will make during training.\n",
    "* metric_for_best_model: The metric used to decide which model checkpoint is the best. Here it is set to eval_loss, meaning lower evaluation loss is preferred."
   ],
   "id": "f05a72dc16f49d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define Early Stopping Callback Parameters",
   "id": "d7cbd5288f8947fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:32:42.193295Z",
     "start_time": "2024-10-17T06:32:42.190098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "early_stop = EarlyStoppingCallback(early_stopping_patience=1, early_stopping_threshold=.0)"
   ],
   "id": "15fde36be8fb4f4f",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This callback is used to stop training early if the model's performance on the validation set does not improve.\n",
    "* early_stopping_patience: Specifies how many consecutive epochs with no improvement are allowed before stopping. A value of 1 means training will stop after just one epoch of no improvement.\n",
    "* early_stopping_threshold: Minimum improvement in the monitored metric (e.g., loss) required to continue training. A value of 0.0 means any improvement is sufficient to keep training."
   ],
   "id": "77bbe2948e841a6b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define Function for Metrics Calculation",
   "id": "e69175021b8141a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:32:53.128282Z",
     "start_time": "2024-10-17T06:32:44.822959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    \n",
    "    return {**accuracy, **f1, **precision, **recall}"
   ],
   "id": "53259c56df479886",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This function computes accuracy, F1 score, precision, and recall for the model's predictions.\n",
    "* Accuracy: Proportion of correct predictions.\n",
    "* F1 Score: Harmonic mean of precision and recall (weighted for imbalanced classes).\n",
    "* Precision: Proportion of positive predictions that were correct.\n",
    "* Recall: Proportion of actual positives that were predicted correctly."
   ],
   "id": "7cc7d93be7543ae6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define Trainer Parameters",
   "id": "5e960b25b169b607"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:32:57.875814Z",
     "start_time": "2024-10-17T06:32:57.853660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_validation_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ],
   "id": "9923a7f83186a44e",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The Trainer class in Hugging Face's transformers library simplifies the training and evaluation of models. It includes parameters for various aspects of training, data handling, evaluation, and optimizations.\n",
    "* model=model: Specifies the model that is being fine-tuned. In this case, a pre-trained model (gemma-2-2b-it) is used for sequence classification.\n",
    "* args=training_args: Contains various training hyperparameters and configurations, defined in the TrainingArguments object.\n",
    "* train_dataset=tokenized_train_dataset: The tokenized training dataset, which has been processed and labeled for the sentiment classification task.\n",
    "* eval_dataset=tokenized_validation_dataset: The dataset used for evaluating the model during training to track performance.\n",
    "* tokenizer=tokenizer: The tokenizer associated with the model, which converts raw text into token IDs that the model can process.\n",
    "* data_collator=data_collator: A function responsible for collecting a batch of data samples and preparing them for the model.\n",
    "* compute_metrics=compute_metrics:  A function that computes evaluation metrics like accuracy, precision, recall, and F1 score.\n",
    "* callbacks=[early_stop]: Specifies callback functions to be executed at certain stages of the training process."
   ],
   "id": "6000355dd07395db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train the Model",
   "id": "9ada459d7a0e42c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T08:05:45.925358Z",
     "start_time": "2024-10-17T06:33:02.923343Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "e44414a6b077d838",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "C:\\Users\\jucse\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\jucse\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7905' max='7905' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7905/7905 1:32:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.499400</td>\n",
       "      <td>0.546998</td>\n",
       "      <td>0.883106</td>\n",
       "      <td>0.877703</td>\n",
       "      <td>0.882679</td>\n",
       "      <td>0.883106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.422700</td>\n",
       "      <td>0.480931</td>\n",
       "      <td>0.880546</td>\n",
       "      <td>0.881077</td>\n",
       "      <td>0.881713</td>\n",
       "      <td>0.880546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.304100</td>\n",
       "      <td>0.597767</td>\n",
       "      <td>0.877986</td>\n",
       "      <td>0.877983</td>\n",
       "      <td>0.878591</td>\n",
       "      <td>0.877986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jucse\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\jucse\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "C:\\Users\\jucse\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\jucse\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7905, training_loss=0.42834574574537476, metrics={'train_runtime': 5562.5396, 'train_samples_per_second': 5.684, 'train_steps_per_second': 1.421, 'total_flos': 1.4641962791786496e+16, 'train_loss': 0.42834574574537476, 'epoch': 3.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The trainer object is used to train the model.",
   "id": "e5ac19f9e06dbb8e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluate the Trained Model",
   "id": "d471445e6049980b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T08:11:27.911348Z",
     "start_time": "2024-10-17T08:08:50.898170Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.evaluate(eval_dataset=tokenized_test_dataset)",
   "id": "fefed48b88334ec0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='732' max='732' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [732/732 02:35]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.520151674747467,\n",
       " 'eval_accuracy': 0.8767076502732241,\n",
       " 'eval_f1': 0.8766970672982001,\n",
       " 'eval_precision': 0.876958392165369,\n",
       " 'eval_recall': 0.8767076502732241,\n",
       " 'eval_runtime': 157.0042,\n",
       " 'eval_samples_per_second': 18.649,\n",
       " 'eval_steps_per_second': 4.662,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The trainer object is used to evaluate the model. Here tokenized_test_dataset is used during evaluation.",
   "id": "d6e90c822664625a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save the Model",
   "id": "467662e21fab3b01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T08:12:24.893412Z",
     "start_time": "2024-10-17T08:12:22.710249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.save_pretrained(\"./fine_tuned_gemma2_2b_it_qlora\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_gemma2_2b_it_qlora\")"
   ],
   "id": "d0cca62beaf67140",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_gemma2_2b_it_qlora\\\\tokenizer_config.json',\n",
       " './fine_tuned_gemma2_2b_it_qlora\\\\special_tokens_map.json',\n",
       " './fine_tuned_gemma2_2b_it_qlora\\\\tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After training, the fine-tuned model is saved for future use",
   "id": "fa8f68667a4e8bde"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T08:14:37.889280Z",
     "start_time": "2024-10-17T08:14:22.959258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fine_tuned_model_name = \"./fine_tuned_gemma2_2b_it_qlora\"\n",
    "\n",
    "fine_tuned_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    fine_tuned_model_name,\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_gemma2_2b_it_qlora\")\n",
    "\n",
    "fine_tuned_model.eval()"
   ],
   "id": "e2c0e847bcff8384",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b49f29411dce417a8417be1dbbf98caf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at google/gemma-2-2b-it and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma2ForSequenceClassification(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2304, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2304, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2304, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2304, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=9216, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (up_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2304, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=9216, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (down_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=9216, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "  )\n",
       "  (score): ModulesToSaveWrapper(\n",
       "    (original_module): Linear(in_features=2304, out_features=3, bias=False)\n",
       "    (modules_to_save): ModuleDict(\n",
       "      (default): Linear(in_features=2304, out_features=3, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sentiment Inference After Fine-Tuning",
   "id": "74c208e4b09f574d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T08:15:05.629570Z",
     "start_time": "2024-10-17T08:14:48.482152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"@united I loved the flight!\"\n",
    "    \n",
    "inputs = tokenizer(input_text, return_tensors='pt', truncation=True)\n",
    "\n",
    "\n",
    "outputs = fine_tuned_model(**inputs)\n",
    "\n",
    "predicted_logits = outputs.logits\n",
    "predicted_class_id = torch.argmax(predicted_logits, dim=1).item()\n",
    "predicted_label = id2label[predicted_class_id]\n",
    "\n",
    "print(predicted_class_id)\n",
    "print(\"Predicted Sentiment:\",predicted_label)"
   ],
   "id": "98e32af708dc0090",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Predicted Sentiment: positive\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T08:15:35.879111Z",
     "start_time": "2024-10-17T08:15:21.747881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"Need to improve your airline service\"\n",
    "    \n",
    "inputs = tokenizer(input_text, return_tensors='pt', truncation=True)\n",
    "\n",
    "\n",
    "outputs = fine_tuned_model(**inputs)\n",
    "\n",
    "predicted_logits = outputs.logits\n",
    "predicted_class_id = torch.argmax(predicted_logits, dim=1).item()\n",
    "predicted_label = id2label[predicted_class_id]\n",
    "\n",
    "print(predicted_class_id)\n",
    "print(\"Predicted Sentiment:\",predicted_label)"
   ],
   "id": "d1236fd250647016",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Predicted Sentiment: negative\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The saved model is loaded for inference, and predictions are made on new texts to classify their sentiment.",
   "id": "19d05e8db8e10357"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "This code performs sentiment classification on airline-related text data using Google's Gemma-2-2b-it model fine-tuned with QLoRA (Quantized LoRA). It leverages Hugging Face Transformers, PyTorch, bitsandbytes, and PEFT libraries to load, fine-tune, and evaluate a transformer model on text data, and applies quantization to optimize model performance on GPUs."
   ],
   "id": "91f7d7f8e08c78a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Limitations\n",
    "* Overfitting\n",
    "* Accuracy less than 90%\n",
    "* Usage of small variant of the pre-trained model"
   ],
   "id": "a152c486e299f612"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ways to Overcome Limitations\n",
    "* Regularization: Increase weight decay or apply stronger regularization techniques.\n",
    "* Dropout: Adjust the dropout rate in LoRA (lora_dropout) to prevent overfitting, especially if overfitting occurs.\n",
    "* Data Augmentation: Use text data augmentation techniques to add diversity to the training set.\n",
    "* Cross-Validation: Perform cross-validation to ensure the model generalizes well across different data splits.\n",
    "* Class Imbalance: Handle class imbalance by using class weights in the loss function or oversampling underrepresented classes.\n",
    "* Batch Size and Learning Rate: Experiment with larger batch sizes or reduce the learning rate to stabilize training.\n",
    "* Early Stopping: Continue using early stopping but consider adjusting the patience to prevent premature stopping."
   ],
   "id": "123bfb5d0d472dbb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
